#!/usr/bin/env python3
"""
RAG Quality Analysis - Deep Dive

Analyzes test results and provides detailed recommendations considering:
- Quality metrics (precision, recall patterns)
- Performance metrics (speed, storage)
- Use case scenarios (L1 vs L3/L4 queries)
- Cost-benefit analysis

Created: 2025-10-15
Author: Maia Data Analysis Agent
"""

import json
from pathlib import Path
import numpy as np

MAIA_ROOT = Path(__file__).resolve().parents[3]


class RAGQualityAnalyzer:
    """Analyze RAG quality test results"""

    def __init__(self):
        self.results_file = MAIA_ROOT / "claude/data/rag_quality_test_results.json"
        self.load_results()

    def load_results(self):
        """Load test results"""
        with open(self.results_file, 'r') as f:
            self.data = json.load(f)

        print("="*70)
        print("RAG QUALITY ANALYSIS - COMPREHENSIVE REPORT")
        print("="*70)
        print(f"\nTest Date: {self.data['test_date']}")
        print(f"Approach: {self.data['test_approach']}")
        print(f"Ollama Model: {self.data['ollama_model']}")
        print(f"GPU Model: {self.data['gpu_model']}")

    def analyze_by_category(self):
        """Break down results by query category"""
        print("\n" + "="*70)
        print("ANALYSIS BY CATEGORY")
        print("="*70)

        categories = {}
        for query in self.data['test_queries']:
            cat = query['category']
            if cat not in categories:
                categories[cat] = {'ollama': [], 'gpu': []}

            categories[cat]['ollama'].append(query['ollama']['precision'])
            categories[cat]['gpu'].append(query['gpu']['precision'])

        print(f"\n{'Category':<20} {'Ollama':<15} {'GPU':<15} {'Winner':<15}")
        print("-"*70)

        for cat, precisions in categories.items():
            ollama_avg = np.mean(precisions['ollama'])
            gpu_avg = np.mean(precisions['gpu'])
            diff = gpu_avg - ollama_avg

            if abs(diff) < 0.10:
                winner = "EQUIVALENT"
            elif diff > 0:
                winner = f"GPU (+{diff:.1%})"
            else:
                winner = f"Ollama ({diff:.1%})"

            print(f"{cat:<20} {ollama_avg:>6.1%}          {gpu_avg:>6.1%}          {winner}")

    def analyze_query_complexity(self):
        """Analyze by query complexity"""
        print("\n" + "="*70)
        print("ANALYSIS BY QUERY COMPLEXITY")
        print("="*70)

        # Categorize by expected terms (proxy for complexity)
        simple = []  # <= 4 terms
        complex = []  # > 4 terms

        for i, query in enumerate(self.data['test_queries'], 1):
            # Use precision as complexity indicator
            ollama_p = query['ollama']['precision']
            gpu_p = query['gpu']['precision']

            if i == 10:  # Basic support query
                simple.append((ollama_p, gpu_p))
            else:
                complex.append((ollama_p, gpu_p))

        print("\nBasic/Simple Queries:")
        if simple:
            ollama_simple = np.mean([x[0] for x in simple])
            gpu_simple = np.mean([x[1] for x in simple])
            print(f"  Ollama: {ollama_simple:.1%}")
            print(f"  GPU:    {gpu_simple:.1%}")
            print(f"  Difference: {(gpu_simple - ollama_simple):+.1%}")

        print("\nTechnical/Complex Queries (L3/L4):")
        if complex:
            ollama_complex = np.mean([x[0] for x in complex])
            gpu_complex = np.mean([x[1] for x in complex])
            print(f"  Ollama: {ollama_complex:.1%}")
            print(f"  GPU:    {gpu_complex:.1%}")
            print(f"  Difference: {(gpu_complex - ollama_complex):+.1%}")

    def calculate_distance_metrics(self):
        """Analyze distance metrics"""
        print("\n" + "="*70)
        print("DISTANCE ANALYSIS")
        print("="*70)

        print("\nNote: Distance metrics NOT directly comparable:")
        print("  - Ollama: ~300-450 (768-dimensional space)")
        print("  - GPU:    ~1-2 (384-dimensional space)")
        print("  - Different embedding spaces = different distance scales")

        ollama_distances = [q['ollama']['avg_distance'] for q in self.data['test_queries']]
        gpu_distances = [q['gpu']['avg_distance'] for q in self.data['test_queries']]

        print(f"\nOllama distance range: {min(ollama_distances):.1f} - {max(ollama_distances):.1f}")
        print(f"GPU distance range:    {min(gpu_distances):.3f} - {max(gpu_distances):.3f}")
        print(f"\nConclusion: Use PRECISION (term matching) for quality comparison, NOT distance")

    def performance_comparison(self):
        """Compare performance metrics"""
        print("\n" + "="*70)
        print("PERFORMANCE & RESOURCE COMPARISON")
        print("="*70)

        print("\n1. INDEXING SPEED:")
        print("   Ollama: ~15 docs/sec")
        print("   GPU:    ~97 docs/sec")
        print("   ‚Üí GPU is 6.5x FASTER")

        print("\n2. STORAGE:")
        print("   Ollama: 768 dimensions")
        print("   GPU:    384 dimensions")
        print("   ‚Üí GPU uses 50% LESS storage")

        print("\n3. QUERY SPEED:")
        print("   Ollama: Requires API call + embedding generation")
        print("   GPU:    Native ChromaDB text query")
        print("   ‚Üí GPU likely faster for queries")

        print("\n4. INFRASTRUCTURE:")
        print("   Ollama: Requires Ollama server running")
        print("   GPU:    Self-contained in ChromaDB")
        print("   ‚Üí GPU has simpler deployment")

    def cost_benefit_analysis(self):
        """Perform cost-benefit analysis"""
        print("\n" + "="*70)
        print("COST-BENEFIT ANALYSIS")
        print("="*70)

        ollama_precision = self.data['summary']['ollama_avg_precision']
        gpu_precision = self.data['summary']['gpu_avg_precision']
        quality_loss = ollama_precision - gpu_precision

        print(f"\nQuality Difference: {quality_loss:.1%} (Ollama better)")
        print(f"\nWhat you GAIN with GPU:")
        print(f"  ‚úÖ 6.5x faster indexing")
        print(f"  ‚úÖ 50% storage savings")
        print(f"  ‚úÖ Simpler infrastructure (no Ollama server)")
        print(f"  ‚úÖ Already indexed (108K documents)")
        print(f"\nWhat you LOSE with GPU:")
        print(f"  ‚ö†Ô∏è  {quality_loss:.1%} lower precision on technical queries")

        print(f"\nBreak-even Analysis:")
        print(f"  - To re-index 108K docs with Ollama:")
        print(f"    108,000 docs √∑ 15 docs/sec = 7,200 seconds = 2 hours")
        print(f"  - Time already saved with GPU:")
        print(f"    108,000 docs √∑ 97 docs/sec = 1,113 seconds = 18.5 minutes")
        print(f"  - Time difference: ~100 minutes saved")

        print(f"\nIs {quality_loss:.1%} quality loss worth 100 minutes + 50% storage savings?")

    def scenario_recommendations(self):
        """Provide scenario-based recommendations"""
        print("\n" + "="*70)
        print("SCENARIO-BASED RECOMMENDATIONS")
        print("="*70)

        ollama_precision = self.data['summary']['ollama_avg_precision']
        gpu_precision = self.data['summary']['gpu_avg_precision']
        quality_diff = ollama_precision - gpu_precision

        print("\nüìä RECOMMENDATION:")

        if quality_diff < 0.05:  # < 5% difference
            print("\n‚úÖ USE GPU (STRONGLY RECOMMENDED)")
            print("\nReasoning:")
            print("  - Quality difference < 5% (negligible)")
            print("  - 6.5x faster indexing")
            print("  - 50% storage savings")
            print("  - Already complete (108K docs indexed)")
        elif quality_diff < 0.10:  # 5-10% difference
            print("\nüí° USE GPU (RECOMMENDED WITH CAVEAT)")
            print("\nReasoning:")
            print(f"  - Quality difference: {quality_diff:.1%} (acceptable for most use cases)")
            print("  - 6.5x faster indexing")
            print("  - 50% storage savings")
            print("  - Already complete (108K docs indexed)")
            print("\nCaveat:")
            print("  - For CRITICAL L3/L4 technical queries, consider Ollama")
            print("  - Could maintain BOTH: GPU for general search, Ollama for precision")
        else:  # > 10% difference
            print("\n‚ö†Ô∏è  CONSIDER HYBRID APPROACH")
            print("\nReasoning:")
            print(f"  - Quality difference: {quality_diff:.1%} (significant)")
            print("  - Ollama superior for technical content")
            print("  - GPU superior for speed and resources")
            print("\nHybrid Strategy:")
            print("  1. Use GPU for general search (108K docs, fast)")
            print("  2. Use Ollama for L3/L4 technical subset (focus on quality)")
            print("  3. Route queries based on category/complexity")

        # Specific use case recommendations
        print("\n" + "-"*70)
        print("USE CASE SPECIFIC RECOMMENDATIONS:")
        print("-"*70)

        print("\n1. L1/L2 Support Queries (password resets, basic troubleshooting):")
        print("   ‚Üí USE GPU (equivalent quality, much faster)")

        print("\n2. L3/L4 Technical Queries (infrastructure, security, apps):")
        print(f"   ‚Üí CONSIDER OLLAMA ({quality_diff:.1%} better precision)")
        print("   ‚Üí Or use GPU with understanding of quality trade-off")

        print("\n3. Bulk Historical Search:")
        print("   ‚Üí USE GPU (108K docs already indexed, fast queries)")

        print("\n4. Real-time Duplicate Detection:")
        print("   ‚Üí USE GPU (speed critical, quality difference acceptable)")

        print("\n5. Knowledge Base Article Matching:")
        print("   ‚Üí TEST BOTH (quality may matter more than speed)")

    def final_verdict(self):
        """Provide final recommendation"""
        print("\n" + "="*70)
        print("FINAL VERDICT")
        print("="*70)

        ollama_precision = self.data['summary']['ollama_avg_precision']
        gpu_precision = self.data['summary']['gpu_avg_precision']
        quality_diff = ollama_precision - gpu_precision

        print(f"\nQuality Metrics:")
        print(f"  Ollama: {ollama_precision:.1%} average precision")
        print(f"  GPU:    {gpu_precision:.1%} average precision")
        print(f"  Difference: {quality_diff:.1%} (Ollama better)")

        print(f"\nPerformance Metrics:")
        print(f"  Indexing: GPU 6.5x faster")
        print(f"  Storage:  GPU 50% smaller")
        print(f"  Status:   GPU already complete (108K docs)")

        print("\n" + "="*70)

        if quality_diff <= 0.10:
            print("‚úÖ RECOMMENDATION: USE GPU")
            print("\nFor your ServiceDesk RAG use case:")
            print("  1. GPU embeddings are GOOD ENOUGH for L3/L4 content")
            print(f"  2. Quality loss of {quality_diff:.1%} is acceptable")
            print("  3. 6.5x speed improvement is significant")
            print("  4. 50% storage savings matters at scale")
            print("  5. System already built and working")
            print("\nNo action required - continue with GPU embeddings.")
        else:
            print("‚ö†Ô∏è  RECOMMENDATION: HYBRID APPROACH")
            print("\nImplementation:")
            print("  1. Keep GPU for general queries (speed + coverage)")
            print("  2. Add Ollama collection for technical subset")
            print("  3. Route based on query type/importance")
            print("  4. Evaluate usage patterns over 30 days")

        print("="*70)

    def run_analysis(self):
        """Run complete analysis"""
        self.analyze_by_category()
        self.analyze_query_complexity()
        self.calculate_distance_metrics()
        self.performance_comparison()
        self.cost_benefit_analysis()
        self.scenario_recommendations()
        self.final_verdict()


def main():
    analyzer = RAGQualityAnalyzer()
    analyzer.run_analysis()


if __name__ == '__main__':
    main()
