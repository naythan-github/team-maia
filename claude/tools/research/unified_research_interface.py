#!/usr/bin/env python3
"""
Unified Research Interface - KAI-Enhanced Research System
=======================================================

Creates a unified interface that leverages:
1. Smart Research Decision Engine (60-95% token savings)
2. KAI GraphRAG semantic search capabilities
3. Predictive context loading for research patterns
4. Intelligent research retention vs. refresh strategy

This interface becomes the single entry point for all research needs,
replacing scattered research agents with a coordinated intelligent system.
"""

import json
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import logging

# Import KAI capabilities
# Core utility import from general domain
try:
    import sys
    from pathlib import Path
    general_tools_dir = Path(__file__).parent.parent / "ðŸ› ï¸_general"
    sys.path.insert(0, str(general_tools_dir))
    try:
        from path_manager import get_path_manager
    finally:
        if str(general_tools_dir) in sys.path:
            sys.path.remove(str(general_tools_dir))
except ImportError:
    # Graceful fallback for missing path_manager
        def get_path_manager(): return None
try:
    # Cross-domain import via emoji resolver
    import sys
    from pathlib import Path
    emoji_tools_dir = Path(__file__).parent.parent / "ðŸ› ï¸_general"
    sys.path.insert(0, str(emoji_tools_dir))
    try:
        from kai_integration_manager import kai_enhanced_query, kai_system_status
    finally:
        if str(emoji_tools_dir) in sys.path:
            sys.path.remove(str(emoji_tools_dir))
except ImportError:
    # Graceful fallback for missing kai_integration_manager
    kai_enhanced_query = None
    kai_system_status = None
# Cross-domain import via emoji resolver
try:
    import sys
    from pathlib import Path
    emoji_tools_dir = Path(__file__).parent.parent / "ðŸ“Š_data"
    sys.path.insert(0, str(emoji_tools_dir))
    try:
        from graphrag_enhanced_knowledge_graph import quick_graphrag_query
    finally:
        if str(emoji_tools_dir) in sys.path:
            sys.path.remove(str(emoji_tools_dir))
except ImportError:
    # Graceful fallback for missing graphrag_enhanced_knowledge_graph
        quick_graphrag_query = None
# Cross-domain import via emoji resolver
try:
    import sys
    from pathlib import Path
    emoji_tools_dir = Path(__file__).parent.parent / "ðŸ› ï¸_general"
    sys.path.insert(0, str(emoji_tools_dir))
    try:
        from predictive_context_loader import ContextDomain, quick_predict_context
    finally:
        if str(emoji_tools_dir) in sys.path:
            sys.path.remove(str(emoji_tools_dir))
except ImportError:
    # Graceful fallback for missing predictive_context_loader
    class ContextDomain: pass
    quick_predict_context = None
    KAI_AVAILABLE = False
    logging.warning("KAI capabilities not available - falling back to traditional research")

# Import Smart Research Manager
try:
    # Same domain import - direct reference
    from smart_research_manager import SmartResearchManager, StabilityTier, RefreshTrigger
    SMART_RESEARCH_AVAILABLE = True
except ImportError:
    # Graceful fallback for missing smart_research_manager
    class SmartResearchManager: pass
    class StabilityTier: pass
    class RefreshTrigger: pass
    SMART_RESEARCH_AVAILABLE = False
    logging.warning("Smart Research Manager not available - running without cache optimization")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)


class ResearchRequest(Enum):
    """Types of research requests"""
    COMPANY_ANALYSIS = "company_analysis"
    CONCEPT_RESEARCH = "concept_research"
    COMPETITIVE_ANALYSIS = "competitive_analysis"
    MARKET_RESEARCH = "market_research"
    TECHNOLOGY_RESEARCH = "technology_research"
    FINANCIAL_ANALYSIS = "financial_analysis"
    CAREER_RESEARCH = "career_research"


@dataclass
class ResearchQuery:
    """Structured research query with KAI enhancements"""
    query_id: str
    request_type: ResearchRequest
    subject: str
    specific_questions: List[str] = None
    context_domain: str = "research"
    priority: int = 5  # 1-10, 10 = highest
    max_age_hours: int = 24  # Cache tolerance
    use_kai: bool = True
    use_smart_cache: bool = True
    include_sources: bool = True
    depth_level: int = 2  # 1=basic, 2=standard, 3=comprehensive
    refresh_triggers: List[str] = None

    def __post_init__(self):
        if self.specific_questions is None:
            self.specific_questions = []
        if self.refresh_triggers is None:
            self.refresh_triggers = []


@dataclass
class ResearchResult:
    """Enhanced research result with KAI metadata"""
    query_id: str
    subject: str
    content: str
    sources: List[str]
    confidence_score: float
    tokens_used: int
    tokens_saved: int = 0
    cache_hit: bool = False
    kai_enhanced: bool = False
    graphrag_used: bool = False
    processing_time_ms: float = 0
    refresh_date: datetime = None
    stability_tier: str = "dynamic"
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.refresh_date is None:
            self.refresh_date = datetime.now()


class UnifiedResearchInterface:
    """
    KAI-Enhanced Unified Research Interface

    Provides intelligent research coordination leveraging:
    - Smart Research Decision Engine for cache optimization
    - KAI GraphRAG for semantic search and synthesis
    - Predictive context loading for faster responses
    - Intelligent routing between cached and live research
    """

    def __init__(self):
        self.path_manager = get_path_manager()
        self.research_db_path = self.path_manager.get_path('data') / 'unified_research.db'

        # Initialize components
        self.smart_research_manager = None
        if SMART_RESEARCH_AVAILABLE:
            try:
                self.smart_research_manager = SmartResearchManager()
                logger.info("âœ… Smart Research Decision Engine loaded")
            except Exception as e:
                logger.warning(f"âš ï¸ Smart Research Manager initialization failed: {e}")

        # Initialize database
        self._initialize_database()

        # Performance tracking
        self.stats = {
            'total_queries': 0,
            'cache_hits': 0,
            'kai_queries': 0,
            'tokens_saved_total': 0,
            'avg_response_time_ms': 0.0
        }

        logger.info("ðŸ”¬ Unified Research Interface initialized")

    def _initialize_database(self):
        """Initialize unified research database"""
        with sqlite3.connect(self.research_db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS unified_research (
                    query_id TEXT PRIMARY KEY,
                    subject TEXT NOT NULL,
                    request_type TEXT NOT NULL,
                    content TEXT NOT NULL,
                    sources TEXT NOT NULL,  -- JSON array
                    confidence_score REAL NOT NULL,
                    tokens_used INTEGER NOT NULL,
                    tokens_saved INTEGER DEFAULT 0,
                    cache_hit BOOLEAN DEFAULT FALSE,
                    kai_enhanced BOOLEAN DEFAULT FALSE,
                    graphrag_used BOOLEAN DEFAULT FALSE,
                    processing_time_ms REAL DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    refresh_date TIMESTAMP NOT NULL,
                    stability_tier TEXT DEFAULT 'dynamic',
                    metadata TEXT DEFAULT '{}'
                )
            ''')

            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_subject ON unified_research(subject)
            ''')

            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_request_type ON unified_research(request_type)
            ''')

    async def research(self, query: ResearchQuery) -> ResearchResult:
        """
        Main research entry point with intelligent routing

        Decision flow:
        1. Check Smart Research cache for existing results
        2. If cache miss or stale, use KAI GraphRAG for semantic search
        3. Synthesize results using predictive context
        4. Cache results with appropriate stability tier
        """
        start_time = datetime.now()
        self.stats['total_queries'] += 1

        logger.info(f"ðŸ” Research query: {query.subject} (Type: {query.request_type.value})")

        # Step 1: Check smart cache
        cached_result = await self._check_smart_cache(query)
        if cached_result:
            logger.info(f"âœ… Cache hit: {query.subject}")
            self.stats['cache_hits'] += 1
            cached_result.processing_time_ms = (datetime.now() - start_time).total_seconds() * 1000
            return cached_result

        # Step 2: Perform KAI-enhanced research
        result = await self._perform_kai_research(query)

        # Step 3: Cache result with smart retention
        await self._cache_result_with_retention(result)

        # Step 4: Update stats
        processing_time = (datetime.now() - start_time).total_seconds() * 1000
        result.processing_time_ms = processing_time
        self._update_stats(processing_time)

        logger.info(f"ðŸŽ¯ Research complete: {query.subject} ({processing_time:.0f}ms)")
        return result

    async def _check_smart_cache(self, query: ResearchQuery) -> Optional[ResearchResult]:
        """Check Smart Research cache for existing results"""
        if not query.use_smart_cache or not self.smart_research_manager:
            return None

        try:
            # Check if we have cached research for this subject
            with sqlite3.connect(self.research_db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    SELECT * FROM unified_research
                    WHERE subject = ? AND request_type = ?
                    ORDER BY created_at DESC LIMIT 1
                ''', (query.subject, query.request_type.value))

                row = cursor.fetchone()
                if not row:
                    return None

                # Check if result is still fresh
                refresh_date = datetime.fromisoformat(row[13])  # refresh_date column
                max_age = timedelta(hours=query.max_age_hours)

                if datetime.now() - refresh_date < max_age:
                    # Return cached result
                    result = ResearchResult(
                        query_id=row[0],
                        subject=row[1],
                        content=row[3],
                        sources=json.loads(row[4]),
                        confidence_score=row[5],
                        tokens_used=row[6],
                        tokens_saved=row[7],
                        cache_hit=True,
                        kai_enhanced=bool(row[9]),
                        graphrag_used=bool(row[10]),
                        refresh_date=refresh_date,
                        stability_tier=row[14],
                        metadata=json.loads(row[15])
                    )

                    # Calculate tokens saved from cache hit
                    estimated_tokens = self._estimate_research_tokens(query)
                    result.tokens_saved = estimated_tokens
                    self.stats['tokens_saved_total'] += estimated_tokens

                    return result

        except Exception as e:
            logger.warning(f"Cache check failed: {e}")

        return None

    async def _perform_kai_research(self, query: ResearchQuery) -> ResearchResult:
        """Perform KAI-enhanced research"""
        result = ResearchResult(
            query_id=query.query_id,
            subject=query.subject,
            content="",
            sources=[],
            confidence_score=0.0,
            tokens_used=0
        )

        try:
            if KAI_AVAILABLE and query.use_kai:
                # Use KAI for semantic research
                logger.info("ðŸ§  Using KAI for enhanced research")

                # Build research prompt
                research_prompt = self._build_research_prompt(query)

                # Use GraphRAG for semantic search
                if query.context_domain:
                    graphrag_context = await self._get_graphrag_context(query)
                    if graphrag_context:
                        result.graphrag_used = True
                        research_prompt += f"\n\nRelevant Context:\n{graphrag_context}"

                # Perform KAI-enhanced query
                kai_response = kai_enhanced_query(research_prompt, query.context_domain)

                result.content = kai_response
                result.kai_enhanced = True
                result.confidence_score = 0.8  # High confidence for KAI results
                result.tokens_used = self._estimate_research_tokens(query)
                self.stats['kai_queries'] += 1

            else:
                # Fallback to traditional research
                logger.info("ðŸ“š Using traditional research methods")
                result.content = await self._traditional_research(query)
                result.confidence_score = 0.6
                result.tokens_used = self._estimate_research_tokens(query)

            # Extract sources (simplified for now)
            result.sources = self._extract_sources(result.content)

            # Determine stability tier
            result.stability_tier = self._determine_stability_tier(query).value

        except Exception as e:
            logger.error(f"Research failed: {e}")
            result.content = f"Research failed: {str(e)}"
            result.confidence_score = 0.0

        return result

    def _build_research_prompt(self, query: ResearchQuery) -> str:
        """Build comprehensive research prompt"""
        prompt_parts = [
            f"Research Topic: {query.subject}",
            f"Research Type: {query.request_type.value}",
            f"Depth Level: {query.depth_level}/3"
        ]

        if query.specific_questions:
            prompt_parts.append("Specific Questions:")
            for i, question in enumerate(query.specific_questions, 1):
                prompt_parts.append(f"{i}. {question}")

        if query.include_sources:
            prompt_parts.append("Please include credible sources and references.")

        return "\n".join(prompt_parts)

    async def _get_graphrag_context(self, query: ResearchQuery) -> Optional[str]:
        """Get relevant context using GraphRAG"""
        try:
            if not KAI_AVAILABLE:
                return None

            # Use GraphRAG to find relevant context
            graphrag_result = quick_graphrag_query(
                query.subject,
                context_type=query.context_domain,
                max_chunks=5
            )

            if graphrag_result and graphrag_result.confidence_score > 0.5:
                return graphrag_result.synthesized_context

        except Exception as e:
            logger.warning(f"GraphRAG context retrieval failed: {e}")

        return None

    async def _traditional_research(self, query: ResearchQuery) -> str:
        """Fallback traditional research implementation"""
        # This would integrate with existing research agents
        # For now, return a placeholder response
        return f"Traditional research completed for: {query.subject}\n\nThis would integrate with existing research agents and web search capabilities."

    def _extract_sources(self, content: str) -> List[str]:
        """Extract source URLs and references from content"""
        # Simplified source extraction
        import re
        urls = re.findall(r'https?://[^\s<>"{}|\\^`[\]]+', content)
        return list(set(urls))

    def _determine_stability_tier(self, query: ResearchQuery) -> StabilityTier:
        """Determine information stability tier for caching"""
        if not SMART_RESEARCH_AVAILABLE:
            return StabilityTier.DYNAMIC

        # Map request types to stability tiers
        stability_mapping = {
            ResearchRequest.COMPANY_ANALYSIS: StabilityTier.STRATEGIC,
            ResearchRequest.CONCEPT_RESEARCH: StabilityTier.FOUNDATION,
            ResearchRequest.COMPETITIVE_ANALYSIS: StabilityTier.STRATEGIC,
            ResearchRequest.MARKET_RESEARCH: StabilityTier.DYNAMIC,
            ResearchRequest.TECHNOLOGY_RESEARCH: StabilityTier.STRATEGIC,
            ResearchRequest.FINANCIAL_ANALYSIS: StabilityTier.DYNAMIC,
            ResearchRequest.CAREER_RESEARCH: StabilityTier.STRATEGIC
        }

        return stability_mapping.get(query.request_type, StabilityTier.DYNAMIC)

    async def _cache_result_with_retention(self, result: ResearchResult):
        """Cache result with smart retention policy"""
        try:
            with sqlite3.connect(self.research_db_path) as conn:
                conn.execute('''
                    INSERT OR REPLACE INTO unified_research
                    (query_id, subject, request_type, content, sources, confidence_score,
                     tokens_used, tokens_saved, cache_hit, kai_enhanced, graphrag_used,
                     processing_time_ms, refresh_date, stability_tier, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    result.query_id,
                    result.subject,
                    "unknown",  # We'd need to track this
                    result.content,
                    json.dumps(result.sources),
                    result.confidence_score,
                    result.tokens_used,
                    result.tokens_saved,
                    result.cache_hit,
                    result.kai_enhanced,
                    result.graphrag_used,
                    result.processing_time_ms,
                    result.refresh_date.isoformat(),
                    result.stability_tier,
                    json.dumps(result.metadata)
                ))

        except Exception as e:
            logger.warning(f"Failed to cache result: {e}")

    def _estimate_research_tokens(self, query: ResearchQuery) -> int:
        """Estimate tokens that would be used for research"""
        # Base estimate based on depth level
        base_tokens = {1: 5000, 2: 10000, 3: 20000}[query.depth_level]

        # Adjust for question count
        question_multiplier = 1 + (len(query.specific_questions) * 0.2)

        return int(base_tokens * question_multiplier)

    def _update_stats(self, processing_time: float):
        """Update performance statistics"""
        # Update average response time
        current_avg = self.stats['avg_response_time_ms']
        total_queries = self.stats['total_queries']
        self.stats['avg_response_time_ms'] = (
            (current_avg * (total_queries - 1) + processing_time) / total_queries
        )

    def get_research_stats(self) -> Dict[str, Any]:
        """Get comprehensive research statistics"""
        cache_hit_rate = 0.0
        if self.stats['total_queries'] > 0:
            cache_hit_rate = (self.stats['cache_hits'] / self.stats['total_queries']) * 100

        return {
            'total_queries': self.stats['total_queries'],
            'cache_hit_rate_percent': cache_hit_rate,
            'kai_queries': self.stats['kai_queries'],
            'tokens_saved_total': self.stats['tokens_saved_total'],
            'avg_response_time_ms': self.stats['avg_response_time_ms'],
            'kai_available': KAI_AVAILABLE,
            'smart_research_available': SMART_RESEARCH_AVAILABLE
        }


# Global instance and convenience functions
_research_interface = None

def get_unified_research_interface() -> UnifiedResearchInterface:
    """Get global research interface instance"""
    global _research_interface
    if _research_interface is None:
        _research_interface = UnifiedResearchInterface()
    return _research_interface


async def quick_research(subject: str,
                        request_type: ResearchRequest = ResearchRequest.CONCEPT_RESEARCH,
                        questions: List[str] = None,
                        depth: int = 2) -> ResearchResult:
    """Quick research interface"""
    interface = get_unified_research_interface()

    query = ResearchQuery(
        query_id=f"quick_{int(datetime.now().timestamp())}",
        request_type=request_type,
        subject=subject,
        specific_questions=questions or [],
        depth_level=depth
    )

    return await interface.research(query)


# Example usage
if __name__ == "__main__":
    async def main():
        interface = UnifiedResearchInterface()

        # Test company research
        result = await quick_research(
            "Microsoft Azure cloud strategy",
            ResearchRequest.COMPANY_ANALYSIS,
            ["What are their key competitive advantages?", "Recent strategic partnerships?"],
            depth=2
        )

        print(f"Research Result: {result.subject}")
        print(f"Confidence: {result.confidence_score}")
        print(f"KAI Enhanced: {result.kai_enhanced}")
        print(f"Cache Hit: {result.cache_hit}")
        print(f"Content: {result.content[:200]}...")

        # Print stats
        stats = interface.get_research_stats()
        print(f"\nStats: {json.dumps(stats, indent=2)}")

    asyncio.run(main())
