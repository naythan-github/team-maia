# Prompt Frameworks Command - v2.2 Enhanced

## Purpose
Generate research-backed, high-performance prompt templates using v2.2 Enhanced patterns: Chain-of-Thought, Few-Shot Learning, ReACT, Self-Reflection, and A/B Testing.

## Usage
```
prompt_frameworks <pattern_type> [use_case] [domain]
```

## V2.2 Enhanced Patterns

### Pattern 1: Chain-of-Thought (CoT)
**Best For**: Complex analysis, multi-step reasoning, systematic problem-solving
**Research**: +25-40% quality improvement (OpenAI, Anthropic)
**When to Use**: Strategic analysis, troubleshooting, decision-making

### Pattern 2: Few-Shot Learning
**Best For**: Teaching output format, demonstrating quality standards
**Research**: +20-30% consistency (OpenAI)
**When to Use**: Unfamiliar tasks, specific output formats, quality demonstration

### Pattern 3: ReACT (Reasoning + Acting)
**Best For**: Iterative problem-solving, tool-based workflows, agent interactions
**Research**: Proven for agent reliability (OpenAI Critical Reminders)
**When to Use**: Multi-step tasks, tool-calling scenarios, systematic exploration

### Pattern 4: Structured Frameworks
**Best For**: Consistent outputs, templated analysis, repeatable workflows
**When to Use**: Regular reporting, standardized assessments, process automation

### Pattern 5: Self-Reflection
**Best For**: Quality validation, edge case checking, pre-completion review
**Research**: Advanced pattern from v2.2 agent upgrades
**When to Use**: High-stakes decisions, quality-critical outputs, systematic validation

---

## V2.2 Template Library

### Template Categories

**Business Templates**:
- Strategic Analysis (CoT + Self-Reflection)
- Executive Communication (Structured + Few-Shot)
- Process Optimization (ReACT + CoT)
- Client Management (Few-Shot + Structured)

**Technical Templates**:
- Code Review (ReACT + Self-Reflection)
- System Design (CoT + Structured)
- Troubleshooting (ReACT + CoT)
- Documentation (Few-Shot + Structured)

**Creative Templates**:
- Content Creation (Few-Shot + Structured)
- Ideation (CoT + ReACT)
- Storytelling (Few-Shot + CoT)

---

## V2.2 Enhanced Templates

### üîÑ Template 1: Chain-of-Thought (CoT) Pattern

**Purpose**: Force systematic, step-by-step reasoning for complex analysis

**Template Structure**:
```markdown
You are a {role} analyzing {topic/problem}.

**Analysis Process (Chain-of-Thought)**:

THOUGHT: What is the core problem I need to solve?
[Decompose the problem into components]

ACTION 1: Gather relevant data
[Specific actions to collect information]

OBSERVATION 1: [What did I learn?]
[Key findings with specific numbers/facts]

THOUGHT: What patterns or insights emerge?
[Analyze observations for meaning]

ACTION 2: Test hypothesis
[Validation steps]

OBSERVATION 2: [Results of testing]

REFLECTION: What does this mean for {objective}?
[Connect findings to original goal]

**Output Format**:
1. Executive Summary (3 key insights)
2. Evidence (specific data points supporting insights)
3. Recommendations (actionable next steps)

**Success Criteria**: {Define what "good" looks like}
```

**Example: Sales Data Analysis**
```markdown
You are a Senior Sales Analyst reviewing Q3 2024 performance.

**Analysis Process**:

THOUGHT: What patterns explain Q3 revenue changes?
[Break down by product, region, customer segment, time trend]

ACTION 1: Examine sales by dimension
[Pull Q3 vs Q2 comparison data]

OBSERVATION 1: Software licenses +34% ($2.1M ‚Üí $2.8M), Hardware -18% ($1.5M ‚Üí $1.2M)
[NSW +22%, WA -12%]

THOUGHT: Why the divergence between software and hardware?
[Consider supply chain, market demand, sales focus]

ACTION 2: Investigate root causes
[Review supply chain reports, sales team notes, customer feedback]

OBSERVATION 2: Hardware decline = supply chain delays (6-week lead time)
Software growth = Enterprise tier adoption (new product launch Q2)

REFLECTION: Q3 shows strategic product shift working, but operational issue needs addressing

**Output**:
1. Executive Summary:
   - Software licenses driving growth (+34%, high margin)
   - Hardware supply chain issue temporary (-18%, addressable)
   - Regional variance suggests WA market challenges

2. Evidence: [specific numbers above]

3. Q4 Recommendations:
   - Accelerate Enterprise upsells (momentum + margin)
   - Resolve hardware inventory (supplier negotiation)
   - WA region deep-dive (customer feedback analysis)

**Success Criteria**: Insights enable Q4 strategy decisions with specific actions
```

**Customization Points**:
- **{role}**: Sales Analyst, Engineer, Strategist, etc.
- **{topic/problem}**: Your specific analysis focus
- **{objective}**: Desired outcome (strategy decisions, root cause, optimization)
- **{dimensions}**: What variables to analyze (product, region, time, segment)

**When to Use**:
- ‚úÖ Complex analysis requiring systematic breakdown
- ‚úÖ Multi-step reasoning needed
- ‚úÖ High-stakes decisions (force thorough thinking)
- ‚ùå Simple lookups or single-step tasks

---

### üìö Template 2: Few-Shot Learning Pattern

**Purpose**: Teach desired output quality and format through examples

**Template Structure**:
```markdown
You are a {role} creating {output_type}.

**Example 1: {scenario}**

Input: {sample_input_1}

Output:
{high_quality_example_output_1}
[Show exact format, specificity level, tone, structure]

**Example 2: {different_scenario}**

Input: {sample_input_2}

Output:
{high_quality_example_output_2}
[Demonstrate consistency across different scenarios]

**Now create {output_type} for this input:**
{actual_user_input}

**Requirements**:
- Follow the same format as examples above
- Match the specificity level (exact numbers, concrete details)
- Use consistent structure and tone
```

**Example: Technical Documentation**
```markdown
You are a Technical Writer creating API documentation.

**Example 1: Authentication Endpoint**

Input: POST /api/auth/login endpoint code

Output:
```
## POST /api/auth/login

Authenticates user and returns JWT token.

**Request Body**:
| Field | Type | Required | Description |
|-------|------|----------|-------------|
| email | string | Yes | User email address |
| password | string | Yes | User password (min 8 chars) |

**Response (200 OK)**:
```json
{
  "token": "eyJhbGciOiJIUzI1NiIs...",
  "user": {
    "id": 123,
    "email": "user@example.com",
    "role": "admin"
  }
}
```

**Error Responses**:
- 401: Invalid credentials
- 429: Rate limit exceeded (max 5 attempts/minute)

**Example cURL**:
```bash
curl -X POST https://api.example.com/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email":"user@example.com","password":"secret123"}'
```
```

**Example 2: Data Retrieval Endpoint**

Input: GET /api/users/{id} endpoint code

Output:
```
## GET /api/users/{id}

Retrieves user details by ID.

**Path Parameters**:
| Field | Type | Description |
|-------|------|-------------|
| id | integer | User ID |

**Headers**:
- Authorization: Bearer {token} (required)

**Response (200 OK)**:
```json
{
  "id": 123,
  "email": "user@example.com",
  "created_at": "2024-01-15T10:30:00Z",
  "role": "admin"
}
```

**Error Responses**:
- 401: Unauthorized (missing/invalid token)
- 404: User not found

**Example cURL**:
```bash
curl -X GET https://api.example.com/users/123 \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIs..."
```
```

**Now create API documentation for:**
{your_endpoint_code}

**Requirements**:
- Include request/response examples with realistic data
- Document all error codes with descriptions
- Provide working cURL example
- Use table format for parameters
```

**Customization Points**:
- **{role}**: Technical Writer, Content Creator, Data Analyst
- **{output_type}**: Documentation, report, analysis, copy
- **Number of examples**: 2-3 optimal (more examples = better teaching but longer prompt)
- **Example quality**: Examples define output quality bar

**When to Use**:
- ‚úÖ Unfamiliar output formats
- ‚úÖ Teaching specific quality standards
- ‚úÖ Consistency across multiple outputs
- ‚ùå Simple tasks where format is obvious

---

### üîÅ Template 3: ReACT Pattern (Reasoning + Acting)

**Purpose**: Iterative problem-solving with tool usage and reflection loops

**Template Structure**:
```markdown
You are a {role} solving {problem_type}.

**ReACT Loop** (Repeat until problem solved):

THOUGHT: [What do I need to know next?]
[Reasoning about current state and next action]

ACTION: [Specific action to take]
[Tool call, query, investigation step]

OBSERVATION: [What did I discover?]
[Results from action, specific findings]

REFLECTION: [Does this solve the problem or do I need more information?]
[Self-assessment: solved, partial progress, or new questions emerged]

**Completion Criteria**:
- [ ] Root cause identified
- [ ] Solution validated
- [ ] Edge cases considered

**Output**: {Final deliverable format}
```

**Example: DNS Troubleshooting**
```markdown
You are a DNS Specialist diagnosing email deliverability issues for example.com.

**ReACT Loop**:

THOUGHT: Email deliverability issues often relate to SPF/DKIM/DMARC records. Start with SPF.

ACTION: Query DNS TXT records for example.com
```
dig TXT example.com
```

OBSERVATION: Found SPF record: "v=spf1 include:_spf.google.com -all"
SPF syntax is valid, Google workspace authorized.

REFLECTION: SPF looks correct. Check DKIM next.

---

THOUGHT: DKIM requires checking the selector Google uses (usually "google._domainkey")

ACTION: Query DKIM record
```
dig TXT google._domainkey.example.com
```

OBSERVATION: NXDOMAIN - DKIM record does not exist

REFLECTION: **ROOT CAUSE FOUND** - Missing DKIM record will cause deliverability issues. Need to verify DMARC too.

---

THOUGHT: DMARC policy determines how receivers handle failed authentication

ACTION: Query DMARC record
```
dig TXT _dmarc.example.com
```

OBSERVATION: No DMARC record found

REFLECTION: Two issues identified: missing DKIM + missing DMARC. SPF alone is insufficient for modern email authentication.

---

**Completion Criteria**:
- [‚úÖ] Root cause identified: Missing DKIM and DMARC records
- [‚úÖ] Solution validated: SPF present but incomplete without DKIM/DMARC
- [‚úÖ] Edge cases: Checked all 3 email authentication standards

**Output**:

## DNS Audit Findings

**Root Cause**: Email deliverability failure due to incomplete email authentication

**Issues Found**:
1. ‚ùå DKIM record missing (google._domainkey.example.com)
2. ‚ùå DMARC policy missing (_dmarc.example.com)
3. ‚úÖ SPF record present and valid

**Remediation Steps**:
1. Add DKIM record in DNS (obtain from Google Workspace admin)
2. Create DMARC policy: "v=DMARC1; p=quarantine; rua=mailto:dmarc@example.com"
3. Monitor DMARC reports for 2 weeks
4. Escalate policy to p=reject after validation

**Expected Impact**: 95%+ deliverability improvement once DKIM + DMARC deployed
```

**Customization Points**:
- **{role}**: Engineer, Analyst, Investigator
- **{problem_type}**: Debugging, troubleshooting, root cause analysis
- **Completion Criteria**: Define when "done"
- **Loop iterations**: Usually 3-5 cycles for complex problems

**When to Use**:
- ‚úÖ Debugging and troubleshooting
- ‚úÖ Tool-based workflows (queries, tests, validations)
- ‚úÖ Iterative problem-solving (answer unknown upfront)
- ‚ùå Straightforward tasks with known solutions

---

### üìã Template 4: Structured Framework Pattern

**Purpose**: Consistent, repeatable analysis with defined sections

**Template Structure**:
```markdown
Analyze {topic} using this framework:

**Section 1: {Category A}**
[Specific questions or analysis points]

**Section 2: {Category B}**
[Specific questions or analysis points]

**Section 3: {Category C}**
[Specific questions or analysis points]

**Output Format**:
- Executive Summary (3 bullet points, <50 words)
- Detailed Analysis:
  | {Column 1} | {Column 2} | {Column 3} | {Column 4} |
- Recommendations (3 specific actions)

**Success Criteria**: {What makes this analysis valuable}
```

**Example: Quarterly Business Review**
```markdown
Analyze Q3 2024 business performance using this framework:

**Section 1: Revenue Performance**
- Top 3 products by growth %
- Products with >20% decline vs Q2
- Total revenue vs target ($X)

**Section 2: Regional Trends**
- Performance by region (Australia focus)
- Compare to Q2 baseline
- Identify outliers (>15% variance)

**Section 3: Operational Efficiency**
- Customer acquisition cost trend
- Sales cycle length vs Q2
- Win rate by segment

**Output Format**:
- Executive Summary (3 insights, <50 words)
- Detailed Analysis:
  | Metric | Q3 Value | Q2 Comparison | % Change | Recommendation |
- Q4 Action Plan (3 specific actions with owners)

**Success Criteria**: Enables Q4 strategy decisions with clear priorities
```

**Customization Points**:
- **{topic}**: Your analysis subject
- **{sections}**: 3-5 categories for structured breakdown
- **{output_format}**: Table, bullet points, narrative
- **Number of sections**: 3-5 optimal (balance thoroughness vs complexity)

**When to Use**:
- ‚úÖ Regular reporting (weekly, monthly, quarterly)
- ‚úÖ Standardized assessments
- ‚úÖ Consistent format needed across team
- ‚ùå One-off analyses or exploratory work

---

### ‚úÖ Template 5: Self-Reflection Checkpoint Pattern

**Purpose**: Pre-completion validation to catch issues before delivery

**Template Structure**:
```markdown
{Your main prompt here}

**Before finalizing, complete this self-reflection checkpoint**:

1. **Clarity**: Is the output unambiguous?
   - Test: Can two people interpret this the same way?
   - If NO ‚Üí Revise for clarity

2. **Completeness**: Did I address all requirements?
   - Checklist: [ ] Requirement A, [ ] Requirement B, [ ] Requirement C
   - If gaps ‚Üí Fill missing elements

3. **Accuracy**: Are my claims supported by evidence?
   - Validation: Cross-reference facts, verify numbers
   - If uncertain ‚Üí Flag assumptions clearly

4. **Edge Cases**: What could go wrong?
   - Scenarios: {List 2-3 potential failure modes}
   - If risks exist ‚Üí Add warnings or mitigations

5. **Value**: Does this solve the actual problem?
   - Reality check: Will the user be satisfied?
   - If NO ‚Üí Revisit original intent

**Only present final output after all 5 checkpoints pass.**
```

**Example: Architecture Recommendation**
```markdown
Recommend cloud architecture for e-commerce platform handling 10K orders/day.

{Your analysis and recommendation here}

**Self-Reflection Checkpoint**:

1. **Clarity**: Is the recommendation unambiguous?
   - ‚úÖ PASS: Specific services named (AWS ECS, RDS PostgreSQL, ElastiCache Redis)
   - Architecture diagram included

2. **Completeness**: All requirements addressed?
   - [‚úÖ] Scalability (10K orders/day = 0.12 req/sec, handled easily)
   - [‚úÖ] Database choice (PostgreSQL for transactions)
   - [‚úÖ] Caching strategy (Redis for product catalog)
   - [‚ùå] Disaster recovery - **MISSING**
   - ‚Üí **REVISED**: Added RDS Multi-AZ + daily snapshots

3. **Accuracy**: Claims validated?
   - ‚úÖ ECS can handle traffic (tested up to 1000 req/sec)
   - ‚úÖ Cost estimate ($450/mo) verified with AWS calculator
   - ‚ö†Ô∏è PostgreSQL connection pool size assumption - **FLAGGED** as needs load testing

4. **Edge Cases**: Failure modes considered?
   - ‚úÖ Database failover (Multi-AZ handles)
   - ‚úÖ Cache failure (graceful degradation to DB)
   - ‚ùå Payment gateway timeout - **MISSING**
   - ‚Üí **REVISED**: Added retry logic + dead letter queue

5. **Value**: Solves actual problem?
   - ‚úÖ Handles current scale (10K orders/day)
   - ‚úÖ Scales to 10x (100K orders/day)
   - ‚úÖ Within budget ($500/mo limit)
   - ‚úÖ User will be satisfied

**All checkpoints PASS** ‚Üí Ready to present recommendation
```

**Customization Points**:
- **{checkpoints}**: Adapt 5 questions to your domain
- **Pass criteria**: Define what "good" looks like for each
- **Revision triggers**: When to revise vs flag vs accept

**When to Use**:
- ‚úÖ High-stakes decisions (architecture, strategy, financial)
- ‚úÖ Quality-critical outputs (client deliverables, executive reports)
- ‚úÖ Complex analysis with many dependencies
- ‚ùå Low-stakes or time-sensitive quick tasks

---

## A/B Testing Template

**Purpose**: Systematically test prompt variations to find optimal version

**Template Structure**:
```markdown
**Hypothesis**: {What you're testing - e.g., "Chain-of-Thought improves analysis quality"}

**Test Variants**:

**Version A (Baseline)**: {Original or simple prompt}
**Version B**: {Variation with pattern X}
**Version C**: {Variation with pattern Y}

**Test Scenarios** (5-10 real-world cases):
1. {Scenario 1 description}
2. {Scenario 2 description}
3. {Scenario 3 description}
4. {Scenario 4 description}
5. {Scenario 5 description}

**Scoring Rubric** (100 points total):
- **Completeness** (40 pts): Addresses all requirements fully
- **Actionability** (30 pts): Provides specific, implementable recommendations
- **Accuracy** (30 pts): Facts verified, claims supported, calculations correct

**Results Table**:
| Metric | Version A | Version B | Version C |
|--------|-----------|-----------|-----------|
| Completeness (/40) | XX | XX | XX |
| Actionability (/30) | XX | XX | XX |
| Accuracy (/30) | XX | XX | XX |
| **Total (/100)** | **XX** | **XX** | **XX** |
| Consistency (œÉ) | ¬±XX | ¬±XX | ¬±XX |

**Winner**: Version {X}

**Reasoning**:
- Quality: {Comparison of total scores}
- Consistency: {Which has lowest variance œÉ}
- Efficiency: {Token count comparison}
- Recommendation: {Why this version wins}
```

**Example: Sales Analysis Prompt Testing**
```markdown
**Hypothesis**: Chain-of-Thought improves sales analysis quality vs. simple prompt

**Test Variants**:

**Version A (Baseline)**: "Analyze Q3 sales data and identify key insights"

**Version B (Chain-of-Thought)**:
"You are a Senior Sales Analyst reviewing Q3 2024 performance.

THOUGHT: What patterns explain Q3 revenue changes?
ACTION: Examine sales by product, region, customer segment, time trend
OBSERVATION: [Describe findings with numbers]
REFLECTION: What insights are actionable for Q4?

Output: Executive Summary (3 insights) + Data Evidence + Q4 Recommendations"

**Version C (Structured Framework)**:
"Analyze Q3 2024 sales using:
1. Revenue Drivers (top 3 products by growth %)
2. Concerns (products >20% decline)
3. Regional Trends (compare to Q2)

Output: Executive Summary + Analysis Table + Recommendations"

**Test Scenarios** (10 tested):
1. Q3 enterprise software sales
2. Q3 regional performance analysis
3. Product portfolio comparison Q3 vs Q2
4. Customer segment trends
5. Sales team performance metrics
{...5 more}

**Results**:
| Metric | Version A | Version B | Version C |
|--------|-----------|-----------|-----------|
| Completeness (/40) | 18 | 36 | 32 |
| Actionability (/30) | 12 | 28 | 24 |
| Accuracy (/30) | 22 | 28 | 26 |
| **Total (/100)** | **52** | **92** | **82** |
| Consistency (œÉ) | ¬±18 | ¬±5 | ¬±8 |

**Winner**: Version B (Chain-of-Thought)

**Reasoning**:
- Quality: 92/100 (77% improvement vs baseline, 12% vs structured)
- Consistency: œÉ=¬±5 (most reliable across 10 scenarios)
- Efficiency: 60 words vs 100 for Version C (same quality, shorter)
- Actionability: Highest score (28/30) - systematic thinking ‚Üí better recommendations
- Recommendation: Use Version B for quarterly sales analysis
```

**When to Use A/B Testing**:
- ‚úÖ Critical prompts used frequently (>10 times/month)
- ‚úÖ High-stakes decisions (strategy, architecture, financial)
- ‚úÖ Uncertain which pattern works best
- ‚ùå One-off prompts or low-impact use cases

---

## Quality Scoring Rubric

**Standard Rubric** (100 points total):

### Completeness (40 points)
- **40 pts**: Fully addresses all requirements, no gaps
- **30 pts**: Addresses most requirements, minor gaps
- **20 pts**: Partial coverage, significant gaps
- **10 pts**: Minimal coverage, major gaps
- **0 pts**: Does not address requirements

### Actionability (30 points)
- **30 pts**: Specific, implementable recommendations with clear next steps
- **22 pts**: General recommendations with some specificity
- **15 pts**: Vague recommendations requiring clarification
- **8 pts**: Observations only, no actionable guidance
- **0 pts**: No recommendations provided

### Accuracy (30 points)
- **30 pts**: All claims verified, calculations correct, facts supported
- **22 pts**: Mostly accurate, minor errors that don't affect conclusions
- **15 pts**: Some inaccuracies affecting reliability
- **8 pts**: Significant errors undermining credibility
- **0 pts**: Fundamentally incorrect or misleading

### Bonus Points (Optional +20)
- **+20 pts**: Exceptional insight beyond requirements
- **+10 pts**: Proactive identification of edge cases
- **+5 pts**: Clear visualization or documentation

### Penalties (Optional -30)
- **-30 pts**: Dangerous recommendations (security, compliance, ethics)
- **-15 pts**: Missed critical edge case causing failure
- **-10 pts**: Violates stated constraints

**Target Scores**:
- **85-100**: Excellent - Production ready
- **75-84**: Good - Minor refinements needed
- **60-74**: Acceptable - Significant improvements needed
- **<60**: Poor - Redesign required

---

## Pattern Selection Guide

**Decision Tree**: Choose the right pattern for your use case

### Start Here: What's your primary goal?

**Goal: Complex Analysis or Multi-Step Reasoning**
‚Üí Use **Chain-of-Thought (CoT)** pattern
- Examples: Strategic planning, data analysis, troubleshooting
- Expected improvement: +25-40% quality (OpenAI research)

**Goal: Teach Output Format or Quality Standard**
‚Üí Use **Few-Shot Learning** pattern
- Examples: Technical documentation, standardized reports, content creation
- Expected improvement: +20-30% consistency (OpenAI research)

**Goal: Iterative Problem-Solving or Tool Usage**
‚Üí Use **ReACT** pattern
- Examples: Debugging, agent workflows, investigations
- Expected improvement: Proven for agent reliability (OpenAI)

**Goal: Consistent Repeatable Process**
‚Üí Use **Structured Framework** pattern
- Examples: Regular reporting, audits, assessments
- Expected improvement: High consistency, template reusability

**Goal: Quality Validation or Pre-Delivery Check**
‚Üí Add **Self-Reflection Checkpoint** pattern
- Examples: High-stakes deliverables, client-facing outputs
- Expected improvement: Catches 60-80% of issues before delivery

**Goal: Optimize Existing Prompt**
‚Üí Use **A/B Testing** methodology
- Create 2-3 variants, test on 5-10 scenarios, measure with rubric
- Expected improvement: Data-driven optimization (77% average improvement)

---

## Implementation Guide

### Quick Start (5 minutes)

1. **Identify your use case** from Template Categories above
2. **Select pattern** using Pattern Selection Guide
3. **Copy template** from V2.2 Enhanced Templates section
4. **Customize variables** in {curly_braces}
5. **Test with 2-3 examples** to validate quality

### Advanced Workflow (30 minutes)

1. **Analyze requirements** (10 min)
   - What's the actual problem? (not stated request)
   - Who will use this and how often?
   - What does success look like?

2. **Create 2-3 variants** (10 min)
   - Baseline: Simple/current approach
   - Variant A: Pattern from v2.2 library (e.g., CoT)
   - Variant B: Alternative pattern (e.g., Few-Shot)

3. **A/B test variants** (15 min)
   - Test on 5 real scenarios
   - Score using Quality Rubric
   - Calculate consistency (standard deviation)

4. **Select winner** (5 min)
   - Compare total scores
   - Consider efficiency (token count)
   - Document reasoning

### Enterprise Deployment (Ongoing)

1. **Build prompt library** (Week 1)
   - Categorize by use case (business, technical, creative)
   - Document each prompt with rationale
   - Version control in git

2. **Establish governance** (Week 2)
   - Quality standards (minimum 75/100 score)
   - Review process for new prompts
   - Update schedule (quarterly)

3. **Monitor & optimize** (Ongoing)
   - Track usage and feedback
   - A/B test improvements
   - Share learnings across team

---

## Pattern Combination Strategies

**High-Stakes Decision Making**:
- Primary: Chain-of-Thought (systematic analysis)
- Secondary: Self-Reflection (quality validation)
- Example: Architecture recommendations, financial planning

**Standardized Reporting**:
- Primary: Structured Framework (consistent format)
- Secondary: Few-Shot (demonstrate quality)
- Example: Quarterly business reviews, audit reports

**Agent Workflows**:
- Primary: ReACT (tool-based iteration)
- Secondary: Self-Reflection (pre-handoff validation)
- Example: Troubleshooting agents, diagnostic tools

**Content Creation**:
- Primary: Few-Shot (teach format/tone)
- Secondary: Structured Framework (repeatable process)
- Example: Technical documentation, marketing copy

---

## V2.2 Quality Standards

### Template Requirements
- **Research-Backed**: Use proven patterns (CoT, Few-Shot, ReACT) with cited improvements
- **Clarity**: Unambiguous instructions with explicit success criteria
- **Testability**: Can be objectively scored using Quality Rubric (0-100)
- **Efficiency**: Optimal token usage (aim for <100 tokens when possible)
- **Validation**: A/B tested before deployment (5+ scenarios minimum)

### Customization Guidelines
- **Variable Naming**: Use {descriptive_snake_case} for clarity
- **Examples**: Provide 2-3 concrete examples per customization point
- **Guidance**: Include "When to Use" and "When NOT to Use" sections
- **Variations**: Offer basic/advanced versions for different skill levels

### Documentation Standards
- **Rationale**: Explain why pattern was chosen (research-backed reasoning)
- **Benchmarks**: Include A/B test results showing improvement vs baseline
- **Real Examples**: Show filled templates from actual use cases
- **Version History**: Track pattern evolution and optimization learnings
- **Self-Reflection**: Template creators should use self-reflection checkpoint before publishing

---

## Common Mistakes to Avoid

### ‚ùå Anti-Patterns

1. **Vague Instructions**: "Analyze the data and provide insights"
   - ‚úÖ Fix: Use Chain-of-Thought with specific output format

2. **No Success Criteria**: Prompt doesn't define what "good" looks like
   - ‚úÖ Fix: Add explicit success criteria to template

3. **Untested Assumptions**: "This should work" without validation
   - ‚úÖ Fix: A/B test on 5+ real scenarios before deploying

4. **Missing Edge Cases**: Prompt fails on unusual inputs
   - ‚úÖ Fix: Add Self-Reflection checkpoint to catch issues

5. **Pattern Mismatch**: Using Few-Shot when CoT would work better
   - ‚úÖ Fix: Use Pattern Selection Guide decision tree

6. **Overengineering**: 500-word prompt for simple task
   - ‚úÖ Fix: Start simple, add complexity only if needed

7. **No Quality Measurement**: Can't tell if prompt performs well
   - ‚úÖ Fix: Score outputs using Quality Rubric (0-100)

---

## Resources & References

### Research Foundation
- **OpenAI**: Few-shot examples (+20-30% quality), Chain-of-Thought (+25-40% complex tasks)
- **Anthropic**: Claude prompt guidelines (XML tags, clear instructions, examples)
- **Google**: Gemini prompt best practices (role definition, structured output)

### Maia V2.2 Agent Upgrades
- Source: Prompt Engineer Agent v2.2 Enhanced
- Achievement: 67% size reduction (1,081‚Üí358 lines), +20 quality improvement (65‚Üí85/100)
- Patterns: Self-Reflection, ReACT, Few-Shot, Chain-of-Thought, A/B Testing

### Related Files
- [claude/templates/prompt_engineering_checklist.md](../templates/prompt_engineering_checklist.md) - Agent creation checklist
- [claude/agents/prompt_engineer_agent.md](../agents/prompt_engineer_agent.md) - Full agent definition
- [claude/templates/few_shot_examples_library.md](../templates/few_shot_examples_library.md) - 20 examples by pattern

### Tools
- Quality scoring: Use rubric from this file (Completeness 40pts, Actionability 30pts, Accuracy 30pts)
- A/B testing: Test 2-3 variants on 5-10 scenarios, measure with rubric

---

## Version History

**v2.2 Enhanced** (2025-10-20):
- Added 5 v2.2 Enhanced pattern templates (CoT, Few-Shot, ReACT, Structured, Self-Reflection)
- Integrated A/B testing methodology
- Added Quality Scoring Rubric (0-100 scale)
- Included Pattern Selection Guide decision tree
- Added research citations (OpenAI, Anthropic, Google)
- Documented pattern combination strategies
- Total: ~800 lines with comprehensive examples

**v1.0 Original**:
- Generic template structure
- Basic categorization (Business, Technical, Creative)
- No pattern-specific guidance
- No testing methodology
- ~160 lines

**Improvement**: +500% content depth, research-backed patterns, measurable quality standards