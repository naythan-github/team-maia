# ServiceDesk ETL Pipeline - Complete Usage Guide

> ‚ö†Ô∏è **ARCHIVED**: This guide documents the SQLite-based ETL pipeline from Phase 127.
> **Current Pipeline**: PostgreSQL-based import via `xlsx_to_postgres.py`
> **See**: `claude/context/knowledge/servicedesk/otc_database_reference.md` for current instructions.
>
> **Key Changes (Jan 2026)**:
> - Database: SQLite ‚Üí PostgreSQL (Docker)
> - Comments columns: `CT-*` ‚Üí `TKTCT-*` format
> - Import tool: `xlsx_to_postgres.py` (upsert with conflict handling)

**Created**: 2025-10-17 (Phase 127)
**Status**: ‚ö†Ô∏è ARCHIVED (Superseded by PostgreSQL pipeline)
**Location**: `/Users/naythandawe/git/maia/claude/tools/sre/`

---

## üìã Quick Start - Process Fresh ServiceDesk Data

**Single Command** (recommended - includes validation):
```bash
cd ~/git/maia
python3 claude/tools/sre/incremental_import_servicedesk.py import \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx
```

This will automatically:
1. ‚úÖ Validate data quality (score ‚â•60 required to proceed)
2. ‚úÖ Clean data (dates, types, text, missing values)
3. ‚úÖ Score final quality (5-dimension assessment)
4. ‚úÖ Import to database (Cloud-touched logic preserved)
5. ‚úÖ Record import metadata

---

## üõ†Ô∏è Pipeline Components

### 1. **Validator** - Pre-Import Quality Check
**File**: `claude/tools/sre/servicedesk_etl_validator.py` (792 lines)
**Purpose**: Validate XLSX data quality before import
**Rules**: 40 validation rules across 6 categories

**Usage**:
```bash
python3 claude/tools/sre/servicedesk_etl_validator.py \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx
```

**Output**:
```
Composite Score: 94.21/100 (üü¢ EXCELLENT)
Decision: ‚úÖ PROCEED

Category Scores:
  Schema Validation: 10/10 (100.0%) ‚úÖ
  Completeness: 32/40 (80.0%) ‚úÖ
  Data Types: 8/8 (100.0%) ‚úÖ
  Business Rules: 7/8 (87.5%) ‚úÖ
  Referential Integrity: 4/4 (100.0%) ‚úÖ
  Text Integrity: 2/2 (100.0%) ‚úÖ
```

**Key Rules**:
- Schema: Required columns present, no unexpected columns
- Completeness: Critical fields populated (CT-COMMENT-ID 94%, TKT-Ticket ID 100%)
- Data Types: Numeric IDs, parseable dates, valid booleans
- Business Rules: Date ranges valid, text lengths reasonable
- Referential Integrity: Foreign keys valid (comments‚Üítickets, timesheets‚Üítickets)
- Text Integrity: No NULL bytes, reasonable newlines

---

### 2. **Cleaner** - Data Standardization
**File**: `claude/tools/sre/servicedesk_etl_cleaner.py` (612 lines)
**Purpose**: Clean and standardize data for import
**Operations**: 5 types (dates, types, text, missing values, defaults)

**Usage**:
```bash
python3 claude/tools/sre/servicedesk_etl_cleaner.py \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx
```

**Output**:
```
‚úÖ Cleaning complete: 22 transformations applied

Total Transformations: 22
Total Records Affected: 4,571,716

Transformations by Operation:
  date_standardization: 3
  type_normalization_int: 5
  type_normalization_float: 1
  type_normalization_bool: 1
  text_cleaning: 5
  missing_value_imputation: 2
  missing_value_check: 5
```

**Cleaning Operations**:
1. **Date Standardization**: DD/MM/YYYY ‚Üí ISO 8601 (dayfirst=True parsing)
2. **Type Normalization**: String IDs ‚Üí Int64, hours ‚Üí float, booleans ‚Üí bool
3. **Text Cleaning**: Whitespace trim, newline normalization, null byte removal
4. **Missing Value Imputation**: Business rules (CT-VISIBLE-CUSTOMER NULL ‚Üí FALSE)
5. **Business Defaults**: Conservative values for missing critical fields

**Complete Audit Trail**: All transformations logged with before/after samples

---

### 3. **Scorer** - Quality Assessment
**File**: `claude/tools/sre/servicedesk_quality_scorer.py` (705 lines)
**Purpose**: Calculate post-cleaning quality score
**Dimensions**: 5 (Completeness, Validity, Consistency, Uniqueness, Integrity)

**Usage**:
```bash
python3 claude/tools/sre/servicedesk_quality_scorer.py \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx
```

**Output**:
```
Composite Score: 90.85/100
Quality Grade: üü¢ EXCELLENT
Recommendation: Production ready

Dimension Breakdown:
  COMPLETENESS: 38.23/40.0 (95.6%)
  VALIDITY: 29.99/30.0 (100.0%)
  CONSISTENCY: 16.39/20.0 (82.0%)
  UNIQUENESS: 3.27/5.0 (65.4%)
  INTEGRITY: 2.96/5.0 (59.2%)
```

**Scoring Algorithm**:
- **Completeness (40 pts)**: Comments 16pts, Tickets 14pts, Timesheets 10pts
- **Validity (30 pts)**: Dates parseable, no invalid ranges, text integrity
- **Consistency (20 pts)**: Temporal logic, type consistency
- **Uniqueness (5 pts)**: Primary keys unique
- **Integrity (5 pts)**: Foreign keys valid, orphan rate acceptable

---

### 4. **Column Mappings** - XLSX‚ÜíDatabase
**File**: `claude/tools/sre/servicedesk_column_mappings.py` (139 lines)
**Purpose**: Map XLSX column names to database fields
**Usage**: Imported by other tools automatically

**Example Mappings**:
```python
COMMENTS_COLUMNS = {
    'CT-COMMENT-ID': 'comment_id',
    'CT-TKT-ID': 'ticket_id',
    'CT-DATEAMDTIME': 'created_time',
    'CT-COMMENT': 'comment_text',
    # ... etc
}
```

---

### 5. **Integrated ETL** - Production Import
**File**: `claude/tools/sre/incremental_import_servicedesk.py` (354 lines, +112 from original)
**Purpose**: Complete end-to-end import with quality validation
**Integration**: Calls validator ‚Üí cleaner ‚Üí scorer ‚Üí import

**Usage**:
```bash
# Standard import (with validation)
python3 claude/tools/sre/incremental_import_servicedesk.py import \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx

# Emergency import (skip validation)
python3 claude/tools/sre/incremental_import_servicedesk.py import \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx \
  --skip-validation

# View import history
python3 claude/tools/sre/incremental_import_servicedesk.py history
```

**Workflow**:
```
STEP 0: Pre-import quality validation
‚îú‚îÄ‚îÄ 0.1 Validator: Baseline quality (94.21/100)
‚îú‚îÄ‚îÄ 0.2 Cleaner: Data standardization (22 transformations)
‚îú‚îÄ‚îÄ 0.3 Scorer: Post-cleaning verification (90.85/100)
‚îî‚îÄ‚îÄ Decision Gate: PROCEED (‚â•60) or HALT (<60)

STEP 1: Import comments (Cloud-touched logic)
STEP 2: Import tickets (Cloud-touched filter)
STEP 3: Import timesheets (Cloud-touched filter)
```

**Quality Gate**: Automatic halt if score <60

---

## üìä Complete Workflow Example

### Scenario: Process Fresh October Data Export

```bash
# 1. Download XLSX files to ~/Downloads/
# 2. Run integrated import (includes validation)
cd ~/git/maia
python3 claude/tools/sre/incremental_import_servicedesk.py import \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx
```

**Expected Output**:
```
================================================================================
SERVICEDESK DATA IMPORT - Enhanced with Quality Validation (Phase 127)
================================================================================

================================================================================
üîç PRE-IMPORT QUALITY VALIDATION (Phase 127)
================================================================================

üìã STEP 0.1: Validating source data quality...
   ‚úÖ Validation score: 94.21/100
   ‚úÖ Validation passed: 94.21/100 >= 60 (threshold)

üßπ STEP 0.2: Cleaning data (date standardization, type normalization)...
   ‚úÖ Applied 22 data cleaning transformations

üìä STEP 0.3: Scoring cleaned data quality...
   ‚úÖ Final quality score: 90.85/100

================================================================================
‚úÖ PRE-IMPORT VALIDATION COMPLETE
================================================================================
   Quality Score: 90.85/100
   Decision: PROCEED WITH IMPORT
================================================================================

üí¨ STEP 1: Importing comments from: /Users/naythandawe/Downloads/comments.xlsx
   Loaded 204,625 rows
   ‚úÖ Filtered to 176,637 rows (July 1+ only)
   üìã Cloud roster: 48 members
   üéØ Identified 10,939 Cloud-touched tickets
   ‚úÖ Keeping ALL comments for Cloud-touched tickets: 108,129 rows
   ‚úÖ Imported as import_id=14
   üìÖ Date range: 2025-07-03 to 2025-10-14

üìã STEP 2: Importing tickets from: /Users/naythandawe/Downloads/tickets.xlsx
   Loaded 652,681 rows
   üéØ Cloud-touched tickets: 10,939 rows
   ‚úÖ Imported as import_id=15
   üìÖ Date range: 2025-07-03 to 2025-10-13

‚è±Ô∏è  STEP 3: Importing timesheets from: /Users/naythandawe/Downloads/timesheets.xlsx
   Loaded 732,959 rows
   ‚úÖ Filtered to 141,062 rows (July 1+ only)
   ‚ö†Ô∏è  Found 128,007 orphaned timesheet entries (90.7%)
   ‚úÖ Imported as import_id=16
   üìÖ Date range: 2025-07-01 to 2026-07-01

================================================================================
‚úÖ IMPORT COMPLETE
================================================================================
   Comments import_id: 14
   Tickets import_id: 15
   Timesheets import_id: 16
   Cloud-touched tickets: 10,939
   Pre-import quality score: 90.85/100
```

---

## üîÑ Re-Index RAG Database (After Import)

After importing fresh data, rebuild the RAG index:

```bash
# Delete old index
rm -rf ~/.maia/servicedesk_rag

# Re-index with fresh data (using local GPU)
python3 ~/git/maia/claude/tools/sre/servicedesk_gpu_rag_indexer.py --index-all
```

**Expected Output**:
```
‚úÖ GPU RAG Indexer initialized
   Model: intfloat/e5-base-v2
   Device: mps (Apple Silicon)
   Batch size: 64

======================================================================
GPU INDEXING ALL COLLECTIONS
======================================================================

Processing collections:
  ‚úÖ comments (108,084 docs) - 10 min
  ‚úÖ descriptions (10,939 docs) - 1 min
  ‚úÖ solutions (10,694 docs) - 30 sec
  ‚úÖ titles (10,939 docs) - 25 sec
  ‚úÖ work_logs (73,273 docs) - 5 min

Total: 213,929 documents indexed in ~15-20 minutes
```

---

## üß™ Testing the Pipeline

### Test 1: Validate Only (No Changes)
```bash
python3 claude/tools/sre/servicedesk_etl_validator.py \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx
```

### Test 2: Clean Only (No Import)
```bash
python3 claude/tools/sre/servicedesk_etl_cleaner.py \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx
```

### Test 3: Score Only (After Cleaning)
```bash
python3 claude/tools/sre/servicedesk_quality_scorer.py \
  ~/Downloads/comments.xlsx \
  ~/Downloads/tickets.xlsx \
  ~/Downloads/timesheets.xlsx
```

### Test 4: Full Pipeline with Dry Run
```bash
# No dry-run mode yet - use --skip-validation and check database before committing
sqlite3 ~/git/maia/claude/data/servicedesk_tickets.db "SELECT COUNT(*) FROM comments"
```

---

## üìà Quality Metrics

### Baseline Results (October 2025 Data):
- **Validator Score**: 94.21/100 (EXCELLENT)
- **Post-Cleaning Score**: 90.85/100 (EXCELLENT)
- **Transformations Applied**: 22
- **Records Affected**: 4,571,716
- **Import Volume**: 260,125 rows (comments, tickets, timesheets)
- **RAG Documents**: 213,929 indexed

### Quality Thresholds:
- **90-100**: EXCELLENT - Production ready
- **80-89**: GOOD - Minor issues acceptable
- **70-79**: ACCEPTABLE - Review warnings
- **60-69**: POOR - Address issues before import
- **<60**: FAILED - DO NOT IMPORT (automatic halt)

---

## ‚ö†Ô∏è Known Issues & Limitations

### 1. In-Place Cleaning
**Issue**: Cleaner modifies source XLSX files in-place
**Workaround**: Backup XLSX files before running
**Future**: Create temporary cleaned files, preserve originals
**Priority**: LOW (acceptable for development)

### 2. ChromaDB Settings Conflicts
**Issue**: Re-indexing may fail with "different settings" error
**Workaround**: Delete `~/.maia/servicedesk_rag/` before re-indexing
**Status**: Resolved by deleting old index first

### 3. Timesheet Entry ID Column
**Issue**: TS-Title is NOT the timesheet entry ID column (0.04% numeric)
**Impact**: Cannot validate timesheet uniqueness correctly
**TODO**: Find correct column in XLSX, update column mappings
**Priority**: LOW (orphaned timesheets expected behavior)

---

## üîß Troubleshooting

### Issue: Validation Fails with Score <60
**Solution**: Review validation report, fix source data issues before import

### Issue: Cleaner Type Conversion Error
**Solution**: Bug fixed in Phase 127 - ensure using latest version (check git commit)

### Issue: RAG Indexing "Different Settings" Error
**Solution**:
```bash
rm -rf ~/.maia/servicedesk_rag
python3 ~/git/maia/claude/tools/sre/servicedesk_gpu_rag_indexer.py --index-all
```

### Issue: Import Fails on Tickets/Timesheets (UnicodeDecodeError)
**Solution**: Bug fixed in Phase 127 - ensure XLSX format support in `incremental_import_servicedesk.py`

---

## üìö Related Documentation

**Phase 127 Documentation**:
- `PHASE_127_DAY_4-5_INTEGRATION_COMPLETE.md` - Complete implementation summary
- `PHASE_127_RECOVERY_STATE.md` - Resume instructions
- `SERVICEDESK_PATTERN_ANALYSIS_OCT_2025.md` - Pattern analysis example

**Project Plan**:
- `SERVICEDESK_ETL_QUALITY_ENHANCEMENT_PROJECT.md` - Full 7-day project plan
- `ROOT_CAUSE_ANALYSIS_ServiceDesk_ETL_Quality.md` - Day 1-2 findings

**Database**:
- Location: `~/git/maia/claude/data/servicedesk_tickets.db` (1.24GB)
- RAG Index: `~/.maia/servicedesk_rag/` (753MB)

---

## üéØ Success Criteria

After processing fresh data, verify:

‚úÖ **Data Quality**: Score ‚â•90/100 (EXCELLENT)
‚úÖ **Import Volume**: Expected row counts (comments ~108K, tickets ~11K, timesheets ~141K)
‚úÖ **RAG Index**: 213K+ documents successfully indexed
‚úÖ **Search Quality**: Test queries return relevant results (0.09-1.03 distance)
‚úÖ **No Errors**: Zero Python exceptions during pipeline execution
‚úÖ **Audit Trail**: All transformations logged with before/after samples

---

**Pipeline Status**: ‚úÖ PRODUCTION READY
**Last Tested**: 2025-10-17 (Phase 127)
**Maintained By**: Maia System / ServiceDesk Manager Agent
