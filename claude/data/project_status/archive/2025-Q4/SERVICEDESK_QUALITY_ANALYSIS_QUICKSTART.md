# ServiceDesk Comment Quality Analysis - Quick Start

**Created**: October 14, 2025
**Status**: âœ… READY TO RUN OVERNIGHT

---

## What This Does

Analyzes 3,300 ServiceDesk comments using your local Ollama LLM to assess:
- **Communication quality** (professionalism, clarity, empathy, actionability)
- **Red flags** (handoffs without context, defensive tone, etc.)
- **Content patterns** (canned responses, meaningful updates, etc.)

**Zero token cost** - runs 100% locally using your existing Ollama + ChromaDB infrastructure.

---

## Quick Start (3 Commands)

### Step 1: Extract Sample (5 minutes)
```bash
cd ~/git/maia
python3 claude/tools/sre/servicedesk_comment_quality_analyzer.py --extract --sample-size 3300
```

**What it does**: Extracts 3,300 comments using stratified sampling:
- 500 from "quick flicker" agents (djain, emoazzam, etc.)
- 500 from high-volume agents
- 500 from tickets with 3+ handoffs
- 500 random baseline
- 1000 team samples (100 per top 10 teams)
- 300 time series (100 per month Jul/Aug/Sep)

**Output**: `claude/data/servicedesk_comment_sample.json`

---

### Step 2: Run LLM Analysis (2-4 hours - OVERNIGHT)
```bash
cd ~/git/maia
nohup python3 claude/tools/sre/servicedesk_comment_quality_analyzer.py --analyze --batch-size 10 > ~/servicedesk_analysis.log 2>&1 &
```

**What it does**: Sends each comment to Ollama (mistral model) for quality assessment
- Processes in batches of 10 to avoid overwhelming Ollama
- Stores results in `comment_quality` table in servicedesk_tickets.db
- Runs in background with output logged

**Check progress**:
```bash
tail -f ~/servicedesk_analysis.log
```

**Or check database**:
```bash
sqlite3 ~/git/maia/claude/data/servicedesk_tickets.db "SELECT COUNT(*) FROM comment_quality"
```

---

### Step 3: Generate Report (Next Morning)
```bash
cd ~/git/maia
python3 claude/tools/sre/servicedesk_comment_quality_analyzer.py --report
```

**What it does**: Generates executive summary with:
- Overall quality distribution (excellent/good/acceptable/poor)
- Top 10 agents by quality score
- Red flag frequency analysis
- Agents requiring intervention (>30% poor quality)

**Output**: `claude/data/SERVICEDESK_QUALITY_REPORT.txt`

---

## Alternative: Run Full Pipeline at Once

```bash
cd ~/git/maia
nohup python3 claude/tools/sre/servicedesk_comment_quality_analyzer.py --full --sample-size 3300 --batch-size 10 > ~/servicedesk_analysis.log 2>&1 &
```

This runs all 3 steps sequentially overnight.

---

## What You'll Get Tomorrow Morning

### 1. Quality Distribution
```
ðŸ“Š OVERALL QUALITY DISTRIBUTION:
   EXCELLENT       425 comments ( 12.9%)
   GOOD           1180 comments ( 35.8%)
   ACCEPTABLE     1240 comments ( 37.6%)
   POOR            455 comments ( 13.8%)
```

### 2. Agent Performance Rankings
```
ðŸ† TOP 10 AGENTS BY QUALITY SCORE:
   Agent           Comments   Quality    Prof      Clarity   Empathy   Action
   -----------------------------------------------------------------------------
   ddignadice      42         4.2        4.5       4.1       4.0       4.2
   wgonzal         18         4.1        4.3       4.2       3.9       4.0
   ...
```

### 3. Red Flag Analysis
```
ðŸš© RED FLAG FREQUENCY:
   no_context_handoff              89 occurrences ( 14.2%)
   closes_without_confirmation     64 occurrences ( 10.2%)
   technical_jargon                52 occurrences (  8.3%)
   ...
```

### 4. Intervention List
```
âš ï¸  AGENTS REQUIRING INTERVENTION (>30% poor quality):
   djain            42 comments,  45.2% poor quality, avg score: 2.1
   anegi            32 comments,  37.5% poor quality, avg score: 2.3
   ...
```

---

## Database Schema Created

The script automatically creates this table:

```sql
comment_quality (
    comment_id TEXT PRIMARY KEY,
    ticket_id TEXT,
    user_name TEXT,
    team TEXT,
    professionalism_score INTEGER,
    clarity_score INTEGER,
    empathy_score INTEGER,
    actionability_score INTEGER,
    quality_score REAL,
    quality_tier TEXT,  -- excellent, good, acceptable, poor
    content_tags TEXT,  -- JSON array
    red_flags TEXT,     -- JSON array
    intent_summary TEXT,
    analysis_timestamp TIMESTAMP
)
```

---

## Phase 1 Findings (SQL Analysis - Already Complete)

These are the findings from today's SQL analysis (no LLM):

### ðŸš¨ Critical Issues Confirmed

1. **23% of closed tickets have ZERO customer communication**
   - 1,834 of 7,969 tickets never got any customer-facing updates
   - Red flag threshold exceeded (target: <15%)

2. **Massive ticket flicking problem**
   - djain: 97.2% quick flick rate (passes tickets in <15 min, 106 of 109 tickets)
   - emoazzam: 93.0% quick flick rate
   - aziadeh: 91.4% quick flick rate (138 of 151 tickets!)
   - Even high-volume agents: msharma (83.2%), jstones (79.9%)

3. **Severe under-communication with customers**
   - msharma: 1.0-1.35 comments per ticket (appears 6 times across teams)
   - bhishanagc: 1.11 comments per ticket
   - rajat: 1.04 comments per ticket
   - Most customers getting ONE update then silence

4. **129 meaningless customer comments**
   - anegi: 32 "NA" / "N/A" / "." comments (24.8% of all meaningless)
   - xianyaoloh: 20 meaningless comments
   - ckemp: 17 meaningless comments

---

## Tomorrow Morning: Combined Analysis

When LLM analysis completes, you'll have:

**SQL Metrics** (quantitative):
- Ticket flicking rates
- Customer communication frequency
- Handoff patterns

**LLM Quality Scores** (qualitative):
- Is djain's communication professional when he DOES comment?
- Are those "NA" comments actually meaningless or contextual?
- Which agents write empathetic, clear updates?

**Combined Insights**:
- Agent profiles: "Ticket Flicker + Poor Quality" vs "Silent Resolver + High Quality"
- Team patterns: Which teams have best communication culture?
- Improvement targets: Specific agents needing training

---

## Troubleshooting

### "Connection refused to localhost:11434"
Ollama isn't running. Start it:
```bash
ollama serve
```

### "Model mistral not found"
Pull the model:
```bash
ollama pull mistral
```

### "Analysis taking too long"
Reduce batch size or sample size:
```bash
python3 claude/tools/sre/servicedesk_comment_quality_analyzer.py --analyze --batch-size 5
```

### Check if still running
```bash
ps aux | grep servicedesk_comment_quality_analyzer
```

### Kill if needed
```bash
pkill -f servicedesk_comment_quality_analyzer
```

---

## Next Steps (Tomorrow)

When you review the results tomorrow, we can:

1. **Deep-dive specific agents** - Look at actual comment examples from poor performers
2. **Create improvement plans** - Targeted training for low-quality communicators
3. **Build dashboard widgets** - Add quality metrics to ServiceDesk dashboard
4. **Monthly tracking** - Rerun analysis monthly to measure improvement
5. **Template library** - Extract "excellent" tier comments as best practice examples

---

## Files Created

- **Script**: `claude/tools/sre/servicedesk_comment_quality_analyzer.py` (LLM analysis tool)
- **Sample**: `claude/data/servicedesk_comment_sample.json` (3,300 comments extracted)
- **Report**: `claude/data/SERVICEDESK_QUALITY_REPORT.txt` (executive summary)
- **Database**: `comment_quality` table in `servicedesk_tickets.db`
- **Log**: `~/servicedesk_analysis.log` (processing output)

---

## Cost & Performance

**Token Cost**: $0 (100% local Ollama processing)
**Processing Time**: 2-4 hours for 3,300 comments
**Database Size**: +5MB for quality results
**Reproducibility**: Can rerun monthly to track improvement

---

## Summary

**Tonight**: Run Step 2 (LLM analysis) in background
**Tomorrow**: Review report, combine with Phase 1 SQL findings, decide on actions

The system is set up to give you comprehensive communication quality insights with zero ongoing costs.
