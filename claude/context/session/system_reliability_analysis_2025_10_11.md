# Maia System Reliability Analysis & Recommendations

**Date**: 2025-10-11
**Analyst**: Maia (SRE Analysis Mode)
**Scope**: Comprehensive reliability assessment of Maia personal AI infrastructure
**Status**: ‚úÖ COMPLETE - Recommendations Ready for Implementation

---

## Executive Summary

**Overall System Health**: ‚ö†Ô∏è **DEGRADED** (66.7% functional, multiple critical gaps)

**Critical Findings**:
- 83 phantom dependencies (18.3% documented tools don't exist)
- 2 single points of failure (whisper, conversation RAG)
- 2 failed weekly services (false positives from calendar logic)
- File permissions issues causing service errors
- No automated recovery mechanisms
- No disaster recovery plan

**Immediate Actions Required**:
1. Implement tool registry (eliminate 83 phantoms)
2. Add redundancy for whisper + conversation RAG
3. Fix file permission errors in downloads router
4. Implement automated health recovery
5. Build disaster recovery procedures

**Estimated Impact**:
- Reliability: 66.7% ‚Üí 95%+ (target SLO)
- Phantom dependencies: 83 ‚Üí 0
- MTTR (Mean Time To Recovery): Unknown ‚Üí <5 minutes
- System health visibility: Manual checks ‚Üí Automated monitoring

---

## Current System State (Baseline Metrics)

### Health Dashboard Summary
```
Component               Status      Score    Target   Gap
=====================  ==========  ======== ======== ========
Continuous Services    ‚úÖ HEALTHY   100.0%   99.9%   +0.1%
Scheduled Services     üî¥ BELOW     66.7%    95.0%   -28.3%
RAG Systems            ‚úÖ HEALTHY   100.0%   99.0%   +1.0%
Tool Dependencies      üî¥ CRITICAL  42.4%    90.0%   -47.6%
UFC Compliance         ‚ùå FAIL      N/A      PASS    FAIL
Overall System         ‚ö†Ô∏è DEGRADED  66.7%    95.0%   -28.3%
```

### Service Availability (17 LaunchAgent Services)
- **Continuous (5/5)**: 100% available ‚úÖ
  - whisper-server, vtt-watcher, downloads-vtt-mover, intelligent-downloads-router, unified-dashboard
- **Scheduled (8/12)**: 66.7% on-schedule üî¥
  - Healthy: 8 (email-rag-indexer, health-monitor, daily-briefing, confluence-sync, etc.)
  - Failed: 2 (system-state-archiver, weekly-backlog-review - false positives, weekly not daily)
  - Unknown: 2 (downloads-organizer-scheduler, whisper-health - no logs, never run)

### RAG Systems (4/4 Active)
- **Conversation RAG**: 3 docs, 0.6 MB, RECENT ‚úÖ
- **Email RAG (Ollama)**: 513 emails, 8.5 MB, FRESH ‚úÖ
- **System State RAG**: 33 docs, 1.4 MB, RECENT ‚úÖ
- **Meeting RAG**: 3 docs, 1.9 MB, FRESH ‚úÖ
- **Total**: 552 documents, 12.31 MB storage

### Resource Utilization
- **Disk Space**: 11GB used / 460GB total (2.4% utilization)
- **Maia Storage**: ~/.maia/ 484MB, ~/git/maia/ 362MB (846MB total)
- **Critical Services Running**: ollama (PID 813), whisper-server (PID 17319)

---

## Critical Reliability Issues

### Issue #1: Phantom Dependencies (Dependency Health 42.4/100)

**Severity**: üî¥ **CRITICAL**
**Impact**: Tool discovery failures, context pollution, trust erosion

**Evidence**:
```
Documented Tools: 454
Actual Tools: 282
Phantom Tools: 83 (18.3% phantom rate)
Tool References: 896 across 104 files
Critical Phantoms: 4 with 5+ references each
```

**Root Causes**:
1. No centralized tool registry (3 discovery methods conflict)
2. Reference sprawl (avg 3.2 mentions per tool)
3. Planned-but-never-built tools remain documented
4. Refactored tools leave stale references
5. Session compression captures incomplete implementations

**Top Phantom Dependencies**:
```
1. systematic_tool_discovery.py (6 refs) - Planned Phase 82, never built
2. cloud_sync_manager.py (11 refs) - Refactored into multiple tools
3. rag_document_connectors.py (4 refs) - Design doc, not implemented
4. enterprise_monitoring_agent.py (5 refs) - Wrong location (agents/ not tools/)
5. conversation_detector.py (3 refs) - In claude/hooks/ not claude/tools/
```

**Impact on Operations**:
- User requests tool, Claude can't find it ‚Üí trust erosion
- Context loading wastes tokens on phantom refs ‚Üí efficiency loss
- Dependency validator shows CRITICAL health ‚Üí blocks save state
- New team members confused by documentation/reality mismatch

**Recommendations**: See Section "Recommendation #1: Centralized Tool Registry"

---

### Issue #2: Single Points of Failure (2 Critical Services)

**Severity**: üî¥ **CRITICAL**
**Impact**: System-wide capability loss if either service fails

**Single Points of Failure Identified**:
1. **whisper_dictation_server.py** (10 references)
   - **Function**: Voice input for hands-free operation
   - **Dependencies**: whisper-cpp binary, microphone access, port 8090
   - **Failure Mode**: No voice input ‚Üí manual typing only
   - **Current State**: Running (PID 17319) since Oct 9
   - **No Redundancy**: Single process, no failover, no health monitoring

2. **conversation_rag_ollama.py** (6 references)
   - **Function**: Knowledge retention across sessions
   - **Dependencies**: Ollama server, ChromaDB, ~/.maia/conversation_rag/
   - **Failure Mode**: No conversation memory ‚Üí context loss
   - **Current State**: Healthy (3 docs, 0.6 MB)
   - **No Redundancy**: Single database, no backups, no replication

**Cascading Failure Risk**:
- Whisper fails ‚Üí voice commands down ‚Üí workflow disruption
- Conversation RAG fails ‚Üí session continuity lost ‚Üí user frustration
- Ollama fails ‚Üí all 4 RAG systems down ‚Üí knowledge inaccessible

**Recommendations**: See Section "Recommendation #2: Redundancy & Failover"

---

### Issue #3: File Permission Errors (Continuous Service Degradation)

**Severity**: ‚ö†Ô∏è **HIGH**
**Impact**: Automated file organization failing, error log pollution

**Evidence**:
```bash
# intelligent_downloads_router_stderr.log (1.2 MB of errors)
PermissionError: [Errno 1] Operation not permitted:
'/Users/YOUR_USERNAME/Downloads/Route53 domains - 2025.10.10 1.xlsx'

# Error pattern repeats for every file
2025-10-11 09:02:49,793 - ERROR - Failed to move Route53 domains...
Traceback: shutil.move() ‚Üí os.rename() ‚Üí PermissionError
```

**Root Cause**:
- macOS Catalina+ Full Disk Access (FDA) restrictions
- LaunchAgent lacks permission to move files from Downloads
- Service continues running but fails silently on every file

**Impact**:
- 100% file move failure rate for downloads router
- 1.2 MB error log accumulation (log bloat)
- User expects automation but files remain in Downloads
- Silent failure (no user notification)

**Affected Service**:
- `com.maia.intelligent-downloads-router` (continuous, PID 35677)
- Status shows HEALTHY (has PID) but functionally DEGRADED

**Recommendations**: See Section "Recommendation #3: macOS Permissions Hardening"

---

### Issue #4: No Automated Recovery Mechanisms

**Severity**: ‚ö†Ô∏è **HIGH**
**Impact**: Manual intervention required for all failures, high MTTR

**Missing Capabilities**:
1. **No automatic service restart**: Failed services stay down until manual intervention
2. **No health-based recovery**: Health monitor detects issues but doesn't fix them
3. **No dependency healing**: Phantom tools detected but not auto-cleaned
4. **No rollback capability**: Bad changes committed without validation
5. **No circuit breakers**: Failing services retry indefinitely

**Evidence**:
- 2 failed services (system-state-archiver, weekly-backlog-review) - manually diagnosed
- File permission errors repeating every minute for days - no recovery
- Phantom dependencies accumulating for 105 phases - no cleanup
- Manual save_state validation added in Phase 103 - not automated

**Current MTTR (Mean Time To Recovery)**:
- **Detection**: Manual (user reports issue or runs health check)
- **Diagnosis**: 15-30 minutes (read logs, run diagnostics)
- **Resolution**: 15-60 minutes (fix + test + deploy)
- **Total MTTR**: 30-90 minutes (unacceptable for critical services)

**Industry Standard SLO**:
- **MTTR Target**: <5 minutes for automated recovery
- **Manual Escalation**: Only for complex issues requiring human judgment

**Recommendations**: See Section "Recommendation #4: Automated Recovery Framework"

---

### Issue #5: No Disaster Recovery Plan

**Severity**: ‚ö†Ô∏è **MEDIUM** (low probability, extreme impact)
**Impact**: Complete data loss if ~/.maia/ or ~/git/maia/ corrupted

**Missing DR Components**:
1. **No automated backups**: RAG databases, logs, configuration not backed up
2. **No backup validation**: Can't verify backup integrity
3. **No recovery testing**: Don't know if backups would actually restore
4. **No RTO/RPO defined**: No recovery time/point objectives
5. **No documented procedures**: Manual recovery steps not written

**Data at Risk**:
```
~/.maia/ (484 MB):
  - conversation_rag/ (3 conversations, 0.6 MB)
  - email_rag_ollama/ (513 emails, 8.5 MB)
  - system_state_rag/ (33 docs, 1.4 MB)
  - meeting_rag/ (3 meetings, 1.9 MB)
  - logs/ (historical service logs)

~/git/maia/ (362 MB):
  - 282 tools (core functionality)
  - 26 agents (specialized expertise)
  - 105 phase documentation
  - Deployment configuration
```

**Failure Scenarios**:
1. **Disk failure**: Complete loss of ~/.maia/ and ~/git/maia/
2. **Ransomware**: Encrypted RAG databases and tools
3. **Accidental deletion**: rm -rf ~/.maia/ (user error)
4. **Corruption**: ChromaDB index corruption (seen in past)
5. **macOS reinstall**: System wipe loses all Maia infrastructure

**Current Mitigation**:
- Git repository (~/git/maia/) backed up to GitHub (code + docs)
- No backup for RAG databases, logs, or runtime data
- Manual exports possible but not automated

**Recommendations**: See Section "Recommendation #5: Disaster Recovery Plan"

---

### Issue #6: UFC Compliance Violations (20 Critical)

**Severity**: ‚ö†Ô∏è **MEDIUM**
**Impact**: Context system health, discoverability issues

**Violations Found**:
```
Total Files: 813
Total Directories: 81
Max Nesting Depth: 6 (preferred 3, max 5)
Critical/High Violations: 20
Medium/Low Warnings: 504
```

**Critical Violations** (depth 6-7):
1. `.pytest_cache/v/cache/nodeids` (depth 7) - test artifacts
2. `team_intelligence/team_intelligence/` (depth 6) - duplicate directory nesting
3. `extensions/archive/2025/security/` (depth 6) - archive structure

**Root Causes**:
- Pytest cache not gitignored (test artifacts in repo)
- Duplicate directory nesting (team_intelligence/team_intelligence/)
- Archive structure predates UFC compliance

**Impact**:
- Slower context loading (deeper traversal)
- Harder discoverability (files buried 6-7 levels deep)
- UFC compliance checker fails ‚Üí blocks save state enforcement

**Recommendations**: See Section "Recommendation #6: UFC Structure Cleanup"

---

## Reliability Best Practices Assessment

### What's Working Well ‚úÖ

**1. Schedule-Aware Health Monitoring (Phase 105)**
- Accurately tracks continuous vs scheduled service health
- Separate SLI/SLO metrics (99.9% continuous, 95% scheduled)
- No false positives for idle scheduled services
- Human-readable health reasons for debugging

**2. RAG System Health (100%)**
- All 4 RAG systems operational
- 552 documents indexed (conversation, email, system state, meetings)
- Fresh data (<24h) for active systems
- Local processing (privacy preserved)

**3. SRE Tools Foundation (Phase 103)**
- Dependency graph validator (430 lines)
- LaunchAgent health monitor (660 lines with Phase 105)
- Pre-flight validation (350 lines)
- Automated health monitor (370 lines orchestrator)

**4. Continuous Service Reliability**
- 5/5 continuous services running (100% SLO met)
- Whisper dictation server uptime: 4+ days
- Ollama server uptime: 4+ days
- VTT watcher processing meetings reliably

### What Needs Improvement üî¥

**1. Tool Discovery & Management**
- 83 phantom dependencies (42.4/100 health score)
- No centralized registry
- 896 scattered references across 104 files
- Manual maintenance, no automation

**2. Failure Detection & Recovery**
- Manual health checks only
- No automated recovery
- MTTR 30-90 minutes (target <5 minutes)
- No circuit breakers or retry logic

**3. Observability & Alerting**
- No real-time dashboards
- No alerting (PagerDuty, Slack, email)
- Logs stored but not monitored
- Health checks run on-demand only

**4. Redundancy & Failover**
- 2 single points of failure (whisper, conversation RAG)
- No backup RAG databases
- No service failover
- No load balancing

**5. Security & Permissions**
- File permission errors unresolved
- No secrets management
- No audit logging
- No access controls

---

## Recommendations (Prioritized)

### Recommendation #1: Centralized Tool Registry (P0 - Critical)

**Goal**: Eliminate 83 phantom dependencies, enable reliable tool discovery

**Implementation**:

**Phase 1: Build Registry (Week 1)**
```json
// claude/data/tool_registry.json
{
  "version": "1.0",
  "last_updated": "2025-10-11T10:00:00Z",
  "tools": [
    {
      "id": "launchagent_health_monitor",
      "path": "claude/tools/sre/launchagent_health_monitor.py",
      "category": "sre",
      "status": "production",
      "description": "Schedule-aware health monitoring",
      "capabilities": ["health-check", "metrics", "slo-tracking"],
      "cli": "python3 {path} --dashboard",
      "dependencies": ["launchctl", "plistlib"],
      "created": "2025-10-11",
      "last_modified": "2025-10-11",
      "references": [
        "claude/context/tools/available.md:66",
        "claude/commands/save_state.md:22"
      ]
    }
    // ... 282 tools
  ]
}
```

**Phase 2: Auto-Discovery Script (Week 1)**
```python
# claude/tools/sre/tool_registry_sync.py

def discover_tools() -> List[Dict]:
    """Scan claude/tools/ for Python files, extract metadata"""
    # Walk filesystem, extract docstrings, detect dependencies

def sync_registry():
    """Update registry from filesystem, mark phantoms"""
    discovered = discover_tools()
    registry = load_registry()
    synced = merge(discovered, registry)
    phantoms = validate_references(synced)
    return save_registry(synced)

def validate_registry() -> int:
    """Validate registry completeness, return exit code"""
    # 0=PASS, 1=WARNING, 2=CRITICAL (phantoms found)
```

**Phase 3: Save State Integration (Week 1)**
```markdown
# claude/commands/save_state.md - Phase 2.1

#### 2.1 Tool Registry Validation ‚≠ê CRITICAL
python3 claude/tools/sre/tool_registry_sync.py --validate

Exit codes:
- 0 = PASS (registry synced, no phantoms)
- 1 = WARNING (minor drift, can proceed)
- 2 = CRITICAL (phantoms detected, fix before commit)

Action if CRITICAL: Run --sync to update, review phantoms
```

**Phase 4: Phantom Cleanup (Week 2)**
- Run sync, identify all 83 phantoms
- For each: build tool OR remove references
- Update documentation to use registry IDs
- Achieve 0 phantoms, 100% coverage

**Success Metrics**:
- Phantom dependencies: 83 ‚Üí 0
- Dependency health: 42.4 ‚Üí 90+ / 100
- Tool references: 896 scattered ‚Üí ~200 registry refs
- Discovery latency: <1s (cached registry)

**Estimated Effort**: 2 weeks (1 week build, 1 week cleanup)

---

### Recommendation #2: Redundancy & Failover (P0 - Critical)

**Goal**: Eliminate 2 single points of failure (whisper, conversation RAG)

**Implementation**:

**Whisper Dictation Redundancy**:
```yaml
Primary: whisper-server (port 8090, ggml-small.bin)
Fallback 1: whisper-cpp CLI (offline fallback)
Fallback 2: macOS native dictation (system fallback)

Health Check:
  - Every 60s: HTTP GET http://127.0.0.1:8090/health
  - If down: restart whisper-server (LaunchAgent KeepAlive)
  - If 3 failures: notify user, switch to fallback

Auto-Recovery:
  # LaunchAgent already has KeepAlive=true
  # Add health monitoring in whisper_health.py
  python3 claude/tools/whisper_health.py --monitor
```

**Conversation RAG Redundancy**:
```yaml
Primary: ~/.maia/conversation_rag/ (ChromaDB)
Backup 1: Automated daily export to JSON
Backup 2: Git-tracked markdown summaries
Backup 3: iCloud/OneDrive sync (encrypted)

Backup Schedule:
  - Every 6 hours: Export RAG to JSON
  - Every day: Sync to cloud storage
  - Every week: Verify restore works

Recovery Procedure:
  1. Detect corruption (health check)
  2. Load most recent JSON backup
  3. Rebuild ChromaDB index
  4. Verify index integrity
  5. Resume operations
```

**Ollama Redundancy** (supports all 4 RAG systems):
```yaml
Primary: Ollama local (port 11434)
Fallback 1: OpenAI API (cloud, costs $$$)
Fallback 2: Anthropic API (cloud, costs $$$)

Health Check:
  - Every 60s: ollama list models
  - If down: restart ollama (brew services restart)
  - If persistent: switch to cloud API

Circuit Breaker:
  - 3 failures ‚Üí OPEN (stop retrying, use fallback)
  - 30s timeout ‚Üí HALF_OPEN (retry primary)
  - 3 successes ‚Üí CLOSED (back to primary)
```

**Success Metrics**:
- Single points of failure: 2 ‚Üí 0
- Whisper uptime: Current ‚Üí 99.9% SLO
- RAG availability: Current ‚Üí 99.9% SLO
- MTTR: Manual ‚Üí <5 minutes (auto-recovery)

**Estimated Effort**: 1 week (health monitors + backup automation)

---

### Recommendation #3: macOS Permissions Hardening (P1 - High)

**Goal**: Fix file permission errors, enable automation to work correctly

**Root Cause**: macOS Full Disk Access (FDA) restrictions block LaunchAgents

**Solution**:

**Step 1: Grant Full Disk Access to Python**
```bash
# System Settings ‚Üí Privacy & Security ‚Üí Full Disk Access
# Add: /usr/bin/python3
# Add: /opt/homebrew/bin/python3 (if using Homebrew)

# Verify:
python3 -c "import os; open(os.path.expanduser('~/Downloads/test.txt'), 'w')"
```

**Step 2: Use Helper Scripts with FDA**
```python
# claude/tools/file_mover_helper.py (with FDA)

import os
import shutil
from pathlib import Path

def move_with_permissions(source: Path, destination: Path):
    """Move file with proper permission handling"""
    try:
        # Try direct move
        shutil.move(str(source), str(destination))
    except PermissionError:
        # Fallback: copy + delete
        shutil.copy2(str(source), str(destination))
        os.remove(str(source))
    except Exception as e:
        # Log and notify user
        log_error(f"Failed to move {source}: {e}")
        notify_user(f"Manual intervention needed: {source}")
```

**Step 3: Update LaunchAgent Configuration**
```xml
<!-- com.maia.intelligent-downloads-router.plist -->
<key>EnvironmentVariables</key>
<dict>
    <key>PATH</key>
    <string>/usr/local/bin:/usr/bin:/bin</string>
    <key>HOME</key>
    <string>/Users/YOUR_USERNAME</string>
</dict>

<!-- Add FDA-aware wrapper -->
<key>ProgramArguments</key>
<array>
    <string>/usr/bin/python3</string>  <!-- System Python with FDA -->
    <string>/Users/YOUR_USERNAME/git/maia/claude/tools/intelligent_downloads_router.py</string>
</array>
```

**Step 4: Error Handling & Notification**
```python
# Add to intelligent_downloads_router.py

def move_file_with_notification(source, dest):
    try:
        move_with_permissions(source, dest)
    except PermissionError as e:
        # Send notification
        os.system(f'''osascript -e 'display notification "Permission denied: {source.name}" with title "Maia Downloads Router"' ''')
        # Log for health monitoring
        log_permission_error(source, dest, e)
```

**Success Metrics**:
- Permission errors: 100% failure ‚Üí 0% failure
- Log bloat: 1.2 MB errors ‚Üí <10 KB (normal logging)
- User satisfaction: Manual file moves ‚Üí automated
- Service health: DEGRADED (hidden) ‚Üí HEALTHY (functional)

**Estimated Effort**: 2 days (FDA setup + error handling + testing)

---

### Recommendation #4: Automated Recovery Framework (P1 - High)

**Goal**: Reduce MTTR from 30-90 minutes to <5 minutes via automation

**Implementation**:

**Component 1: Health Monitor with Recovery**
```python
# claude/tools/sre/health_monitor_with_recovery.py

class HealthMonitorWithRecovery:
    def __init__(self):
        self.monitors = {
            'launchagents': LaunchAgentMonitor(),
            'rag_systems': RAGSystemMonitor(),
            'dependencies': DependencyMonitor(),
            'ollama': OllamaMonitor(),
            'whisper': WhisperMonitor()
        }
        self.recovery_actions = {
            'launchagent_down': self.restart_launchagent,
            'rag_corrupted': self.restore_rag_from_backup,
            'ollama_down': self.restart_ollama,
            'whisper_down': self.restart_whisper,
            'phantom_dependency': self.cleanup_phantom
        }

    def monitor_and_recover(self):
        """Run health checks, auto-recover issues"""
        for name, monitor in self.monitors.items():
            health = monitor.check_health()

            if health.status in ['DEGRADED', 'FAILED']:
                # Attempt recovery
                recovery_action = self.recovery_actions.get(health.issue_type)
                if recovery_action:
                    print(f"üîß Auto-recovering: {health.issue_type}")
                    recovery_action(health)

                    # Verify recovery worked
                    post_health = monitor.check_health()
                    if post_health.status == 'HEALTHY':
                        print(f"‚úÖ Recovery successful: {name}")
                    else:
                        print(f"‚ùå Recovery failed, escalating: {name}")
                        self.escalate_to_user(health)
```

**Component 2: LaunchAgent Auto-Restart**
```python
def restart_launchagent(self, health_data):
    """Restart failed LaunchAgent service"""
    service_name = health_data.service_name

    # Unload
    subprocess.run([
        'launchctl', 'unload',
        f'~/Library/LaunchAgents/{service_name}.plist'
    ])

    # Wait 2s
    time.sleep(2)

    # Reload
    subprocess.run([
        'launchctl', 'load',
        f'~/Library/LaunchAgents/{service_name}.plist'
    ])

    # Verify
    time.sleep(5)
    result = subprocess.run([
        'launchctl', 'list', service_name
    ], capture_output=True)

    return result.returncode == 0
```

**Component 3: RAG Backup & Restore**
```python
def backup_rag_system(rag_name):
    """Export RAG to JSON for recovery"""
    rag_dir = Path.home() / ".maia" / rag_name
    backup_dir = Path.home() / ".maia" / "backups" / rag_name
    backup_dir.mkdir(parents=True, exist_ok=True)

    # Export ChromaDB to JSON
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = backup_dir / f"{rag_name}_{timestamp}.json"

    # Load collection
    client = chromadb.PersistentClient(path=str(rag_dir))
    collection = client.get_collection(rag_name)

    # Export all documents
    data = collection.get()
    with open(backup_file, 'w') as f:
        json.dump(data, f, indent=2)

    return backup_file

def restore_rag_from_backup(health_data):
    """Restore RAG from most recent backup"""
    rag_name = health_data.rag_name
    backup_dir = Path.home() / ".maia" / "backups" / rag_name

    # Find most recent backup
    backups = sorted(backup_dir.glob(f"{rag_name}_*.json"))
    if not backups:
        return False

    latest_backup = backups[-1]

    # Restore
    with open(latest_backup) as f:
        data = json.load(f)

    # Rebuild ChromaDB
    rag_dir = Path.home() / ".maia" / rag_name
    shutil.rmtree(rag_dir, ignore_errors=True)

    client = chromadb.PersistentClient(path=str(rag_dir))
    collection = client.create_collection(rag_name)

    # Re-index documents
    collection.add(
        ids=data['ids'],
        documents=data['documents'],
        metadatas=data['metadatas'],
        embeddings=data['embeddings']
    )

    return True
```

**Component 4: Scheduled Health Monitoring**
```xml
<!-- com.maia.health-monitor-recovery.plist -->
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.maia.health-monitor-recovery</string>

    <key>ProgramArguments</key>
    <array>
        <string>/usr/bin/python3</string>
        <string>/Users/YOUR_USERNAME/git/maia/claude/tools/sre/health_monitor_with_recovery.py</string>
        <string>--monitor-and-recover</string>
    </array>

    <key>StartInterval</key>
    <integer>300</integer>  <!-- Every 5 minutes -->

    <key>StandardOutPath</key>
    <string>/Users/YOUR_USERNAME/.maia/logs/health_monitor_recovery.log</string>
</dict>
</plist>
```

**Success Metrics**:
- MTTR: 30-90 min ‚Üí <5 min (target met)
- Manual interventions: Current ‚Üí <10% (90% auto-recovered)
- Service uptime: 66.7% ‚Üí 95%+ (SLO met)
- Recovery success rate: Target 80% (requires escalation 20%)

**Estimated Effort**: 1 week (build framework + test recovery scenarios)

---

### Recommendation #5: Disaster Recovery Plan (P2 - Medium)

**Goal**: Ensure Maia can recover from catastrophic failures (disk loss, corruption, ransomware)

**Implementation**:

**RTO/RPO Objectives**:
```
Recovery Time Objective (RTO): 4 hours
Recovery Point Objective (RPO): 24 hours

Translation:
- Can restore Maia to working state within 4 hours
- May lose up to 24 hours of RAG data (last backup)
```

**Backup Strategy**:
```yaml
What to Backup:
  1. Git Repository (~/git/maia/)
     - Already backed up to GitHub
     - Frequency: Every commit
     - Storage: GitHub (free, unlimited)

  2. RAG Databases (~/.maia/)
     - conversation_rag/ (3 docs, 0.6 MB)
     - email_rag_ollama/ (513 emails, 8.5 MB)
     - system_state_rag/ (33 docs, 1.4 MB)
     - meeting_rag/ (3 meetings, 1.9 MB)
     - Frequency: Every 6 hours
     - Storage: iCloud Drive (encrypted)

  3. Configuration Files
     - ~/Library/LaunchAgents/com.maia.*.plist
     - ~/.ollama/ (models)
     - ~/.maia/whisper-models/
     - Frequency: On change
     - Storage: Git + iCloud

  4. Logs (selective)
     - ~/.maia/logs/*.log (last 7 days only)
     - Frequency: Daily
     - Storage: Compressed archives in iCloud

What NOT to Backup:
  - Temporary files, caches
  - Test artifacts (.pytest_cache)
  - Large model files (can re-download)
```

**Backup Automation**:
```python
# claude/tools/sre/disaster_recovery_backup.py

def backup_all():
    """Execute full backup of Maia infrastructure"""

    # 1. Backup RAG databases
    backup_rag_systems()

    # 2. Backup configuration
    backup_launchagents()
    backup_ollama_config()

    # 3. Export RAG to JSON (portable format)
    export_rag_to_json()

    # 4. Create manifest (what was backed up, when)
    create_backup_manifest()

    # 5. Sync to iCloud
    sync_to_cloud_storage()

    # 6. Verify backup integrity
    verify_backup_completeness()

def verify_restore():
    """Test restore from backup (in temp directory)"""
    temp_dir = Path("/tmp/maia_restore_test")

    # Download latest backup
    download_from_cloud(temp_dir)

    # Restore RAG databases
    restore_rag_systems(temp_dir)

    # Verify RAG health
    health = check_rag_health(temp_dir)

    # Cleanup
    shutil.rmtree(temp_dir)

    return health.all_healthy()
```

**Backup Schedule (LaunchAgent)**:
```xml
<!-- com.maia.disaster-recovery-backup.plist -->
<key>StartCalendarInterval</key>
<array>
    <dict>
        <key>Hour</key>
        <integer>2</integer>  <!-- 2am -->
        <key>Minute</key>
        <integer>0</integer>
    </dict>
    <dict>
        <key>Hour</key>
        <integer>8</integer>  <!-- 8am -->
        <key>Minute</key>
        <integer>0</integer>
    </dict>
    <dict>
        <key>Hour</key>
        <integer>14</integer>  <!-- 2pm -->
        <key>Minute</key>
        <integer>0</integer>
    </dict>
    <dict>
        <key>Hour</key>
        <integer>20</integer>  <!-- 8pm -->
        <key>Minute</key>
        <integer>0</integer>
    </dict>
</array>
<!-- Runs 4x/day: 2am, 8am, 2pm, 8pm -->
```

**Recovery Procedure**:
```markdown
# Maia Disaster Recovery Runbook

## Scenario 1: Complete System Loss (macOS Reinstall)

### Step 1: Restore Code (15 min)
git clone https://github.com/naythan-orro/maia.git ~/git/maia
cd ~/git/maia

### Step 2: Install Dependencies (30 min)
brew install python3 ollama whisper-cpp
pip3 install -r requirements.txt
ollama pull nomic-embed-text
ollama pull llama3.2:3b

### Step 3: Restore RAG Databases (10 min)
# Download from iCloud
cp -r ~/Library/Mobile\ Documents/com~apple~CloudDocs/Maia-Backups/latest/* ~/.maia/

# Verify integrity
python3 claude/tools/sre/rag_system_health_monitor.py --dashboard

### Step 4: Restore Configuration (10 min)
# LaunchAgents
cp ~/git/maia/config/launchagents/*.plist ~/Library/LaunchAgents/
launchctl load ~/Library/LaunchAgents/com.maia.*.plist

### Step 5: Verify System Health (5 min)
python3 claude/tools/sre/automated_health_monitor.py

### RTO Achieved: ~70 minutes (within 4 hour target)
```

**Success Metrics**:
- Backup frequency: Never ‚Üí Every 6 hours
- Backup verification: Never ‚Üí Weekly automated tests
- Recovery testing: Never ‚Üí Monthly drill
- RTO achievement: Unknown ‚Üí <4 hours
- RPO: Undefined ‚Üí <24 hours

**Estimated Effort**: 1 week (build backup automation + write runbooks + test recovery)

---

### Recommendation #6: UFC Structure Cleanup (P3 - Low)

**Goal**: Achieve UFC compliance (eliminate 20 critical violations)

**Critical Violations to Fix**:

**1. Remove Pytest Cache (depth 7)**
```bash
# Add to .gitignore
echo "**/.pytest_cache/" >> .gitignore
echo "**/__pycache__/" >> .gitignore

# Remove from repo
find claude/tools -name ".pytest_cache" -type d -exec rm -rf {} +
git add .gitignore
git rm -r --cached */.pytest_cache
```

**2. Fix Duplicate Directory Nesting (depth 6)**
```bash
# team_intelligence/team_intelligence/ ‚Üí team_intelligence/
mv claude/context/knowledge/team_intelligence/team_intelligence/* \
   claude/context/knowledge/team_intelligence/

rmdir claude/context/knowledge/team_intelligence/team_intelligence
```

**3. Flatten Archive Structure (depth 6)**
```bash
# extensions/archive/2025/security/ ‚Üí extensions/archive/security_2025/
mv claude/extensions/archive/2025/security claude/extensions/archive/security_2025
rmdir claude/extensions/archive/2025
```

**4. Add UFC Validation to Save State**
```markdown
# claude/commands/save_state.md - Phase 2.3

#### 2.3 UFC Compliance Check
python3 claude/tools/security/ufc_compliance_checker.py --check

Exit codes:
- 0 = PASS (compliant)
- 1 = WARNING (minor violations, can proceed)
- 2 = FAIL (critical violations, fix before commit)

Action if FAIL: Review violations, fix structure
```

**Success Metrics**:
- Critical violations: 20 ‚Üí 0
- Max nesting depth: 7 ‚Üí 5 (compliant)
- UFC compliance: FAIL ‚Üí PASS
- Context loading speed: Baseline ‚Üí 10% faster (less traversal)

**Estimated Effort**: 2 days (cleanup + validation + testing)

---

### Recommendation #7: Observability & Alerting (P3 - Low)

**Goal**: Real-time visibility into system health, proactive alerting

**Implementation** (Future Enhancement):

**Component 1: Real-Time Dashboard**
```python
# claude/tools/unified_dashboard_v2.py

# Add health monitoring page
# http://localhost:8100/health

- Live service status (updates every 30s)
- RAG system health (document counts, freshness)
- Dependency graph visualization
- Recent errors/warnings
- SLI/SLO trends (7-day charts)
```

**Component 2: Slack/Email Alerting**
```python
# claude/tools/sre/health_alerting.py

def send_alert(severity, message):
    """Send alert via Slack/email based on severity"""

    if severity == 'CRITICAL':
        # Immediate notification
        send_slack_alert(message, channel='#maia-critical')
        send_email(to='naythan@orro.com.au', subject='CRITICAL: Maia', body=message)

    elif severity == 'WARNING':
        # Batch notifications (hourly digest)
        queue_alert(message)

    elif severity == 'INFO':
        # Log only, no notification
        log_info(message)
```

**Component 3: Metrics Collection**
```python
# claude/tools/sre/metrics_collector.py

# Collect and store metrics for trends
- Service uptime %
- MTTR per incident
- Phantom dependency count
- RAG query latency
- Tool discovery success rate

# Store in SQLite for historical analysis
# Generate weekly/monthly reports
```

**Success Metrics**:
- Alert latency: Manual checks ‚Üí <60s real-time
- Dashboard uptime: N/A ‚Üí 99% availability
- MTTD (Mean Time To Detect): Unknown ‚Üí <5 minutes
- False positive rate: Target <5% (high signal)

**Estimated Effort**: 2 weeks (build dashboard + alerting + metrics)

---

## Implementation Roadmap

### Phase 1: Critical Reliability (Weeks 1-2) - P0

**Week 1**: Tool Registry + Discovery
- ‚úÖ Build tool_registry.json (seed 282 tools)
- ‚úÖ Build tool_registry_sync.py (discover + validate)
- ‚úÖ Integrate validation into save_state.md
- üéØ Outcome: Stop accumulating phantoms

**Week 2**: Phantom Cleanup
- ‚úÖ Run sync, identify all 83 phantoms
- ‚úÖ For each phantom: build tool OR remove references
- ‚úÖ Update documentation to use registry IDs
- üéØ Outcome: 0 phantoms, dependency health 90+/100

**Metrics**:
- Dependency health: 42.4 ‚Üí 90+ / 100
- Phantom dependencies: 83 ‚Üí 0
- System reliability: 66.7% ‚Üí 80%

---

### Phase 2: Automated Recovery (Weeks 3-4) - P0/P1

**Week 3**: Redundancy & Failover
- ‚úÖ Build health monitors for whisper + ollama
- ‚úÖ Implement RAG backup automation (every 6h)
- ‚úÖ Add circuit breakers + fallback logic
- üéØ Outcome: 0 single points of failure

**Week 4**: Auto-Recovery Framework
- ‚úÖ Build health_monitor_with_recovery.py
- ‚úÖ Implement LaunchAgent auto-restart
- ‚úÖ Add RAG restore from backup
- ‚úÖ Deploy as LaunchAgent (every 5 min)
- üéØ Outcome: MTTR <5 minutes

**Metrics**:
- Single points of failure: 2 ‚Üí 0
- MTTR: 30-90 min ‚Üí <5 min
- System reliability: 80% ‚Üí 95%

---

### Phase 3: Operational Excellence (Weeks 5-6) - P1/P2

**Week 5**: Permissions + DR
- ‚úÖ Fix macOS Full Disk Access issues
- ‚úÖ Build disaster recovery backup automation
- ‚úÖ Write recovery runbooks + test procedures
- ‚úÖ Monthly recovery drill scheduled
- üéØ Outcome: Permission errors 0%, DR plan operational

**Week 6**: UFC Compliance + Observability
- ‚úÖ Clean up 20 UFC violations
- ‚úÖ Add real-time health dashboard
- ‚úÖ Implement basic alerting (Slack)
- üéØ Outcome: UFC compliant, proactive monitoring

**Metrics**:
- Permission errors: 100% failure ‚Üí 0%
- UFC compliance: FAIL ‚Üí PASS
- System reliability: 95% ‚Üí 97%

---

### Phase 4: Advanced Features (Weeks 7-8) - P3

**Optional Enhancements**:
- Distributed tracing for debugging
- Performance profiling + optimization
- Multi-machine deployment support
- Advanced analytics + ML for anomaly detection

**Metrics**:
- System reliability: 97% ‚Üí 99%+ (SLO target)
- Observability maturity: Basic ‚Üí Advanced

---

## Risk Assessment

### Implementation Risks

**Risk #1: Registry Migration Breaks Existing Tools**
- **Probability**: Medium (30%)
- **Impact**: High (tools undiscoverable)
- **Mitigation**: Phased rollout, maintain available.md as fallback for 30 days
- **Rollback**: Revert to available.md if registry fails

**Risk #2: Automated Recovery Causes Cascading Failures**
- **Probability**: Low (10%)
- **Impact**: High (makes problems worse)
- **Mitigation**: Circuit breakers, max retry limits, human escalation
- **Rollback**: Disable auto-recovery LaunchAgent

**Risk #3: Backup Restore Fails During Disaster**
- **Probability**: Medium (20%)
- **Impact**: Critical (data loss)
- **Mitigation**: Monthly restore testing, verify backup integrity
- **Rollback**: Manual recovery from Git + partial data loss acceptable

**Risk #4: macOS Permission Changes Break Services**
- **Probability**: Low (10%)
- **Impact**: Medium (services stop working)
- **Mitigation**: Test FDA changes on non-critical service first
- **Rollback**: Revoke FDA, revert to manual file moves

---

## Success Criteria

### Quantitative Metrics (Target vs Baseline)

```
Metric                        Baseline    Target    Improvement
============================  ==========  ========  ===========
Overall System Health         66.7%       95.0%     +42.4%
Dependency Health Score       42.4/100    90+/100   +113%
Phantom Dependencies          83          0         -100%
Single Points of Failure      2           0         -100%
MTTR (Mean Time To Recovery)  30-90 min   <5 min    -83% to -94%
Service Uptime (Continuous)   100%        99.9%     -0.1% (SLO)
Service On-Time (Scheduled)   66.7%       95.0%     +42.4%
RAG System Availability       100%        99.9%     -0.1% (acceptable)
Permission Error Rate         100%        0%        -100%
UFC Compliance                FAIL        PASS      ‚úÖ
```

### Qualitative Outcomes

**User Experience**:
- ‚úÖ Tools are discoverable (no "can't find tool" errors)
- ‚úÖ System self-heals (fewer manual interventions)
- ‚úÖ Fast recovery from failures (<5 min MTTR)
- ‚úÖ Confidence in reliability (disaster recovery tested)

**Developer Experience**:
- ‚úÖ Easy to add new tools (registry auto-discovery)
- ‚úÖ Clear documentation (tool registry is source of truth)
- ‚úÖ Fast debugging (observability + health monitoring)
- ‚úÖ Preventable errors (save state validation catches issues)

**Operational Maturity**:
- ‚úÖ Proactive monitoring (detect issues before user notices)
- ‚úÖ Automated recovery (90% issues self-heal)
- ‚úÖ Disaster readiness (tested recovery procedures)
- ‚úÖ Compliance (UFC structure enforced)

---

## Appendix A: Current Service Inventory

### Continuous Services (5/5 Running - 100%)
1. **whisper-server** (PID 17319) - Voice dictation
2. **vtt-watcher** (PID 812) - Meeting transcript analysis
3. **downloads-vtt-mover** (PID 826) - VTT file organization
4. **intelligent-downloads-router** (PID 35677) - File routing (has permission issues)
5. **unified-dashboard** (PID 45366) - Health dashboard

### Scheduled Services (8/12 Healthy - 66.7%)

**Healthy (8)**:
1. **email-rag-indexer** (every 1h) - Last ran 5m ago
2. **health-monitor** (every 30m) - Last ran 17m ago
3. **email-question-monitor** (every 6h) - Last ran 4.2h ago
4. **trello-status-tracker** (every 4h) - Last ran 13m ago
5. **email-vtt-extractor** (every 1h) - Last ran 1m ago
6. **daily-briefing** (daily 7am) - Last ran 2.7h ago
7. **confluence-sync** (daily 8am) - Last ran 1.7h ago
8. **sre-health-monitor** (daily 9am) - Last ran 43m ago

**Failed (2) - False Positives**:
9. **system-state-archiver** (Sundays 2am) - Last ran 6.3d ago (HEALTHY, weekly not daily)
10. **weekly-backlog-review** (Sundays 6pm) - Last ran 5.7d ago (HEALTHY, weekly not daily)

**Unknown (2) - Never Run**:
11. **downloads-organizer-scheduler** (every 1m) - No logs exist
12. **whisper-health** (invalid config: every 0s) - No logs exist

---

## Appendix B: RAG System Details

### Conversation RAG
- **Storage**: ~/.maia/conversation_rag/
- **Documents**: 3 conversations
- **Size**: 0.6 MB
- **Freshness**: RECENT
- **Purpose**: Session continuity, knowledge retention
- **Status**: ‚úÖ HEALTHY

### Email RAG (Ollama)
- **Storage**: ~/.maia/email_rag_ollama/
- **Documents**: 513 emails
- **Size**: 8.5 MB
- **Freshness**: FRESH
- **Purpose**: Email search, question answering
- **Status**: ‚úÖ HEALTHY

### System State RAG
- **Storage**: ~/.maia/system_state_rag/
- **Documents**: 33 system state snapshots
- **Size**: 1.4 MB
- **Freshness**: RECENT
- **Purpose**: Historical system state tracking
- **Status**: ‚úÖ HEALTHY

### Meeting RAG
- **Storage**: ~/.maia/meeting_rag/
- **Documents**: 3 meeting transcripts
- **Size**: 1.9 MB
- **Freshness**: FRESH
- **Purpose**: Meeting intelligence, FOB summaries
- **Status**: ‚úÖ HEALTHY

---

## Appendix C: Dependency Health Breakdown

### Phantom Dependency Categories

**Category 1: Planned But Never Built (35 phantoms)**
- Tools documented in design phase but implementation never happened
- Example: systematic_tool_discovery.py (Phase 82 plan, never implemented)

**Category 2: Refactored Without Doc Update (28 phantoms)**
- Tools renamed/restructured, old references remain
- Example: cloud_sync_manager.py (split into multiple tools)

**Category 3: Wrong Location (12 phantoms)**
- Tools exist but in different directory than expected
- Example: conversation_detector.py (in hooks/ not tools/)

**Category 4: Session Compression Artifacts (8 phantoms)**
- Session summaries reference incomplete implementations
- Example: dynamic_context_loader.py (prototype, not deployed)

---

## Conclusion

Maia's current reliability is **DEGRADED (66.7%)** but fixable. The 6-week implementation plan addresses all critical issues:

**Weeks 1-2** (P0): Tool registry eliminates 83 phantoms ‚Üí dependency health 90+/100
**Weeks 3-4** (P0/P1): Auto-recovery + redundancy ‚Üí MTTR <5min, 0 SPOFs
**Weeks 5-6** (P1/P2): Permissions + DR + UFC ‚Üí 95%+ reliability

**Expected Outcome**: **95%+ system reliability**, meeting SLO targets, with automated monitoring and recovery.

**Next Action**: Begin Phase 1 (Tool Registry) implementation.
